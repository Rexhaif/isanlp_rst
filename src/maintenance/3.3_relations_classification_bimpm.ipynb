{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhetorical relations classification used in tree building: Step 3. BiMPM\n",
    "\n",
    "Prepare data and model-related scripts.\n",
    "\n",
    "Evaluate models.\n",
    "\n",
    "Make and evaluate an ansemble for BiMPM and feature rich model.\n",
    "\n",
    "Output:\n",
    " - ``models/relation_predictor_bimpm/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.file_reading import read_edus, read_gold, read_negative, read_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/label_predictor_bimpm'\n",
    "! mkdir $MODEL_PATH\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(MODEL_PATH, 'nlabel_cf_train.tsv')\n",
    "DEV_FILE_PATH = os.path.join(MODEL_PATH, 'nlabel_cf_dev.tsv')\n",
    "TEST_FILE_PATH = os.path.join(MODEL_PATH, 'nlabel_cf_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_labeling'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train_samples['relation'].value_counts(normalize=False).values\n",
    "NUMBER_CLASSES = len(counts)\n",
    "print(\"number of classes:\", NUMBER_CLASSES)\n",
    "print(\"class weights:\")\n",
    "np.round(counts.min() / counts, decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_samples.reset_index()\n",
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'index']].to_csv(TRAIN_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "dev_samples = dev_samples.reset_index()\n",
    "dev_samples[['relation', 'snippet_x', 'snippet_y', 'index']].to_csv(DEV_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "test_samples = test_samples.reset_index()\n",
    "test_samples[['relation', 'snippet_x', 'snippet_y', 'index']].to_csv(TEST_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Core code for ``bimpm_custom_package`` is in ``2.3_structure_classification_custom_bimpm.ipynb``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/bimpm_custom_package/model/multiclass_bimpm.py\n",
    "\n",
    "\"\"\"\n",
    "BiMPM (Bilateral Multi-Perspective Matching) model implementation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Optional, List, Any\n",
    "\n",
    "from overrides import overrides\n",
    "import torch\n",
    "import numpy\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "\n",
    "from allennlp.modules.bimpm_matching import BiMpmMatching\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@Model.register(\"multiclass_bimpm\")\n",
    "class BiMpm(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` augments with additional features the BiMPM model described in `Bilateral Multi-Perspective \n",
    "    Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814>`_ by Zhiguo Wang et al., 2017.\n",
    "    implemented in https://github.com/galsang/BIMPM-pytorch>`_.\n",
    "    Additional features are added before the feedforward classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 matcher_word: BiMpmMatching,\n",
    "                 encoder1: Seq2SeqEncoder,\n",
    "                 matcher_forward1: BiMpmMatching,\n",
    "                 matcher_backward1: BiMpmMatching,\n",
    "                 encoder2: Seq2SeqEncoder,\n",
    "                 matcher_forward2: BiMpmMatching,\n",
    "                 matcher_backward2: BiMpmMatching,\n",
    "                 aggregator: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 dropout: float = 0.1,\n",
    "                 class_weights: list = [],\n",
    "                 encode_together: bool = False,\n",
    "                 encode_lstm: bool = True,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(BiMpm, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "\n",
    "        self.matcher_word = matcher_word\n",
    "\n",
    "        self.encoder1 = encoder1\n",
    "        self.matcher_forward1 = matcher_forward1\n",
    "        self.matcher_backward1 = matcher_backward1\n",
    "\n",
    "        self.encoder2 = encoder2\n",
    "        self.matcher_forward2 = matcher_forward2\n",
    "        self.matcher_backward2 = matcher_backward2\n",
    "\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "        self.encode_together = encode_together\n",
    "        self.encode_lstm = encode_lstm\n",
    "        \n",
    "        matching_dim = self.matcher_word.get_output_dim()\n",
    "        \n",
    "        if self.encode_lstm:\n",
    "            matching_dim += self.matcher_forward1.get_output_dim(\n",
    "            ) + self.matcher_backward1.get_output_dim(\n",
    "            ) + self.matcher_forward2.get_output_dim(\n",
    "            ) + self.matcher_backward2.get_output_dim(\n",
    "            )\n",
    "            \n",
    "        check_dimensions_match(matching_dim, self.aggregator.get_input_dim(),\n",
    "                       \"sum of dim of all matching layers\", \"aggregator input dim\")\n",
    "\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        if class_weights:\n",
    "            self.class_weights = class_weights\n",
    "        else:\n",
    "            self.class_weights = [1.] * self.classifier_feedforward.get_output_dim()\n",
    "            \n",
    "        self.metrics = {\"accuracy\": CategoricalAccuracy()}\n",
    "        \n",
    "        for _class in range(len(self.class_weights)):\n",
    "            self.metrics.update({\n",
    "                f\"f1_rel{_class}\": F1Measure(_class),\n",
    "            })\n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(self.class_weights))\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                premise: Dict[str, torch.LongTensor],\n",
    "                hypothesis: Dict[str, torch.LongTensor],\n",
    "                label: torch.LongTensor = None,\n",
    "                metadata: List[Dict[str, Any]] = None  # pylint:disable=unused-argument\n",
    "               ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            The premise from a ``TextField``\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            The hypothesis from a ``TextField``\n",
    "        label : torch.LongTensor, optional (default = None)\n",
    "            The label for the pair of the premise and the hypothesis\n",
    "        metadata : ``List[Dict[str, Any]]``, optional, (default = None)\n",
    "            Additional information about the pair\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "        \n",
    "        def encode_pair(x1, x2, mask1=None, mask2=None):\n",
    "            _joined_pair: Dict[str, torch.LongTensor] = {}\n",
    "            \n",
    "            for key in premise.keys():\n",
    "                bsz = premise[key].size(0)\n",
    "                x1_len, x2_len = premise[key].size(1), hypothesis[key].size(1)\n",
    "                sep = torch.empty([bsz, 1], dtype=torch.long, device=premise[key].device)\n",
    "                sep.data.fill_(0) # 2 is the id for </s>\n",
    "                \n",
    "                x = torch.cat([premise[key], hypothesis[key]], dim=1)\n",
    "                _joined_pair[key] = x\n",
    "                \n",
    "            x_output = self.dropout(self.text_field_embedder(_joined_pair))\n",
    "            return x_output[:, :x1_len], x_output[:, -x2_len:], mask1, mask2\n",
    "\n",
    "        mask_premise = util.get_text_field_mask(premise)\n",
    "        mask_hypothesis = util.get_text_field_mask(hypothesis)\n",
    "        \n",
    "        if self.encode_together:\n",
    "            embedded_premise, embedded_hypothesis, _, _ = encode_pair(premise, hypothesis)\n",
    "        else:\n",
    "            embedded_premise = self.dropout(self.text_field_embedder(premise))\n",
    "            embedded_hypothesis = self.dropout(self.text_field_embedder(hypothesis))\n",
    "\n",
    "        # embedding and encoding of the premise\n",
    "        encoded_premise1 = self.dropout(self.encoder1(embedded_premise, mask_premise))\n",
    "        encoded_premise2 = self.dropout(self.encoder2(encoded_premise1, mask_premise))\n",
    "\n",
    "        # embedding and encoding of the hypothesis\n",
    "        encoded_hypothesis1 = self.dropout(self.encoder1(embedded_hypothesis, mask_hypothesis))\n",
    "        encoded_hypothesis2 = self.dropout(self.encoder2(encoded_hypothesis1, mask_hypothesis))\n",
    "        \n",
    "        matching_vector_premise: List[torch.Tensor] = []\n",
    "        matching_vector_hypothesis: List[torch.Tensor] = []\n",
    "\n",
    "        def add_matching_result(matcher, encoded_premise, encoded_hypothesis):\n",
    "            # utility function to get matching result and add to the result list\n",
    "            matching_result = matcher(encoded_premise, mask_premise, encoded_hypothesis, mask_hypothesis)\n",
    "            matching_vector_premise.extend(matching_result[0])\n",
    "            matching_vector_hypothesis.extend(matching_result[1])\n",
    "\n",
    "        # calculate matching vectors from word embedding, first layer encoding, and second layer encoding\n",
    "        add_matching_result(self.matcher_word, embedded_premise, embedded_hypothesis)\n",
    "        half_hidden_size_1 = self.encoder1.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward1,\n",
    "                            encoded_premise1[:, :, :half_hidden_size_1],\n",
    "                            encoded_hypothesis1[:, :, :half_hidden_size_1])\n",
    "        add_matching_result(self.matcher_backward1,\n",
    "                            encoded_premise1[:, :, half_hidden_size_1:],\n",
    "                            encoded_hypothesis1[:, :, half_hidden_size_1:])\n",
    "\n",
    "        half_hidden_size_2 = self.encoder2.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward2,\n",
    "                            encoded_premise2[:, :, :half_hidden_size_2],\n",
    "                            encoded_hypothesis2[:, :, :half_hidden_size_2])\n",
    "        add_matching_result(self.matcher_backward2,\n",
    "                            encoded_premise2[:, :, half_hidden_size_2:],\n",
    "                            encoded_hypothesis2[:, :, half_hidden_size_2:])\n",
    "\n",
    "        # concat the matching vectors\n",
    "        matching_vector_cat_premise = self.dropout(torch.cat(matching_vector_premise, dim=2))\n",
    "        matching_vector_cat_hypothesis = self.dropout(torch.cat(matching_vector_hypothesis, dim=2))\n",
    "\n",
    "        # aggregate the matching vectors\n",
    "        aggregated_premise = self.dropout(self.aggregator(matching_vector_cat_premise, mask_premise))\n",
    "        aggregated_hypothesis = self.dropout(self.aggregator(matching_vector_cat_hypothesis, mask_hypothesis))\n",
    "\n",
    "        # encode additional information\n",
    "        #batch_size, _ = aggregated_premise.size()\n",
    "        #encoded_meta = metadata.float().view(batch_size, -1)\n",
    "        \n",
    "        # the final forward layer\n",
    "        logits = self.classifier_feedforward(torch.cat([aggregated_premise, aggregated_hypothesis], dim=-1))\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        output_dict = {'logits': logits, \"probs\": probs}\n",
    "        \n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label)\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Converts indices to string labels, and adds a ``\"label\"`` key to the result.\n",
    "        \"\"\"\n",
    "        predictions = output_dict[\"probs\"].cpu().data.numpy()\n",
    "        argmax_indices = numpy.argmax(predictions, axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
    "                  for x in argmax_indices]\n",
    "        output_dict['label'] = labels\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metrics = {\"accuracy\": self.metrics[\"accuracy\"].get_metric(reset=reset)}\n",
    "        \n",
    "        for _class in range(len(self.class_weights)):\n",
    "            metrics.update({\n",
    "                f\"f1_rel{_class}\": self.metrics[f\"f1_rel{_class}\"].get_metric(reset=reset)[2],\n",
    "            })\n",
    "        \n",
    "        metrics[\"f1_macro\"] = numpy.mean([metrics[f\"f1_rel{_class}\"] for _class in range(len(self.class_weights))])\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate config files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELMo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $MODEL_PATH/config_elmo.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"quora_paraphrase\",\n",
    "    \"lazy\": false,\n",
    "    \"tokenizer\": {\n",
    "      \"type\": \"word\",\n",
    "      \"word_splitter\": {\n",
    "        \"type\": \"just_spaces\"\n",
    "      }\n",
    "    },\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 30,\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"label_predictor_bimpm/nlabel_cf_train.tsv\",\n",
    "  \"validation_data_path\": \"label_predictor_bimpm/nlabel_cf_dev.tsv\",\n",
    "  \"test_data_path\": \"label_predictor_bimpm/nlabel_cf_test.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"multiclass_bimpm\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"class_weights\": [\n",
    "        0.022915, 0.027624, 0.069509, 0.104457, 0.109012, 0.111773,\n",
    "        0.123355, 0.139147, 0.153374, 0.157233, 0.159915, 0.16129 ,\n",
    "        0.191327, 0.202703, 0.288462, 0.337838, 0.347222, 0.535714,\n",
    "        0.630252, 0.757576, 0.806452, 1.0      ],\n",
    "    \"encode_together\": false,\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"elmo\": {\n",
    "                    \"type\": \"elmo_token_embedder\",\n",
    "                    \"options_file\": \"rsv_elmo/options.json\",\n",
    "                    \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "                    \"do_layer_norm\": false,\n",
    "                    \"dropout\": 0.1\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"dropout\": 0.1,\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"padding_index\": 0,\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"gru\",\n",
    "                    \"input_size\": 20,\n",
    "                    \"hidden_size\": 50,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true,\n",
    "                },\n",
    "            },\n",
    "      }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 1024+100,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 1024+100,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 400,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": 400,\n",
    "      \"num_layers\": 1,\n",
    "      \"hidden_dims\": [22],\n",
    "      \"activations\": [\"linear\"],\n",
    "      \"dropout\": [0.0]\n",
    "    },\n",
    "    \"initializer\": [\n",
    "      [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "      [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "    ],\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"bucket\",\n",
    "    \"padding_noise\": 0,\n",
    "    \"sorting_keys\": [[\"premise\", \"num_tokens\"], [\"hypothesis\", \"num_tokens\"]],\n",
    "    \"batch_size\": 4\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 200,\n",
    "    \"cuda_device\": 1,\n",
    "    \"shuffle\": true,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"lr\": 0.001\n",
    "    },\n",
    "    \"type\":\"callback\",\n",
    "    \"callbacks\":[\n",
    "        {\n",
    "            \"type\": \"validate\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"checkpoint\",\n",
    "            \"checkpointer\":{\n",
    "                \"num_serialized_models_to_keep\":1\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gradient_norm_and_clip\", \n",
    "            \"grad_norm\": 5.0\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"track_metrics\",\n",
    "            \"patience\": 20,\n",
    "            \"validation_metric\": \"+f1_macro\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"log_metrics_to_wandb\"\n",
    "        }\n",
    "    ],\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELMo + fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $MODEL_PATH/config_elmo_fasttext.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"quora_paraphrase\",\n",
    "    \"lazy\": false,\n",
    "    \"tokenizer\": {\n",
    "      \"type\": \"word\",\n",
    "      \"word_splitter\": {\n",
    "        \"type\": \"just_spaces\"\n",
    "      }\n",
    "    },\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 30,\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "      },\n",
    "      \"tokens\": {\n",
    "        \"type\": \"single_id\",\n",
    "        \"lowercase_tokens\": true\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"label_predictor_bimpm/nlabel_cf_train.tsv\",\n",
    "  \"validation_data_path\": \"label_predictor_bimpm/nlabel_cf_dev.tsv\",\n",
    "  \"test_data_path\": \"label_predictor_bimpm/nlabel_cf_test.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"multiclass_bimpm\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"class_weights\": [\n",
    "        0.022915, 0.027624, 0.069509, 0.104457, 0.109012, 0.111773,\n",
    "        0.123355, 0.139147, 0.153374, 0.157233, 0.159915, 0.16129 ,\n",
    "        0.191327, 0.202703, 0.288462, 0.337838, 0.347222, 0.535714,\n",
    "        0.630252, 0.757576, 0.806452, 1.0      ],\n",
    "    \"encode_together\": false,\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"tokens\": {\n",
    "                \"type\": \"embedding\",\n",
    "                \"embedding_dim\": 300,\n",
    "                \"pretrained_file\": \"ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\",\n",
    "                \"trainable\": false\n",
    "            },\n",
    "            \"elmo\": {\n",
    "                    \"type\": \"elmo_token_embedder\",\n",
    "                    \"options_file\": \"rsv_elmo/options.json\",\n",
    "                    \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "                    \"do_layer_norm\": false,\n",
    "                    \"dropout\": 0.1\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"dropout\": 0.1,\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"padding_index\": 0,\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"gru\",\n",
    "                    \"input_size\": 20,\n",
    "                    \"hidden_size\": 50,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true,\n",
    "                },\n",
    "            },\n",
    "      }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 1024+100+300,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 1024+100+300,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 400,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": 400,\n",
    "      \"num_layers\": 1,\n",
    "      \"hidden_dims\": [22],\n",
    "      \"activations\": [\"linear\"],\n",
    "      \"dropout\": [0.0]\n",
    "    },\n",
    "    \"initializer\": [\n",
    "      [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "      [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "    ],\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"bucket\",\n",
    "    \"padding_noise\": 0,\n",
    "    \"sorting_keys\": [[\"premise\", \"num_tokens\"], [\"hypothesis\", \"num_tokens\"]],\n",
    "    \"batch_size\": 4\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 200,\n",
    "    \"cuda_device\": 1,\n",
    "    \"shuffle\": true,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"lr\": 0.001\n",
    "    },\n",
    "    \"type\":\"callback\",\n",
    "    \"callbacks\":[\n",
    "        {\n",
    "            \"type\": \"validate\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"checkpoint\",\n",
    "            \"checkpointer\":{\n",
    "                \"num_serialized_models_to_keep\":1\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gradient_norm_and_clip\", \n",
    "            \"grad_norm\": 5.0\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"track_metrics\",\n",
    "            \"patience\": 20,\n",
    "            \"validation_metric\": \"+f1_macro\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"log_metrics_to_wandb\"\n",
    "        }\n",
    "    ],\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv ../../../maintenance_rst/models/label_predictor_bimpm ../../../maintenance_rst/models/label_predictor_bimpm_OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/label_predictor_bimpm ../../../maintenance_rst/models/label_predictor_bimpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scripts for training/prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1. Directly from the config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/train_label_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_label_predictor.sh {bert|elmo} result_30\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"nlabel_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"nlabel_cf_test.tsv\"\n",
    "\n",
    "rm -r label_predictor_bimpm/${RESULT_DIR}/\n",
    "allennlp train -s label_predictor_bimpm/${RESULT_DIR}/ label_predictor_bimpm/config_${METHOD}.json \\\n",
    "    --include-package customization_package\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_bimpm/${RESULT_DIR}/predictions_dev.json label_predictor_bimpm/${RESULT_DIR}/model.tar.gz label_predictor_lstm/${DEV_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_bimpm/${RESULT_DIR}/predictions_test.json label_predictor_bimpm/${RESULT_DIR}/model.tar.gz label_predictor_lstm/${TEST_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on dev&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/eval_label_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_label_predictor.sh {bert|elmo} result_30\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"nlabel_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"nlabel_cf_test.tsv\"\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_bimpm/${RESULT_DIR}/predictions_dev.json label_predictor_bimpm/${RESULT_DIR}/model.tar.gz label_predictor_bimpm/${DEV_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_bimpm/${RESULT_DIR}/predictions_test.json label_predictor_bimpm/${RESULT_DIR}/model.tar.gz label_predictor_bimpm/${TEST_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) predict on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/eval_label_predictor_train.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh eval_label_predictor_train.sh {bert|elmo} result_30\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export TEST_FILE_PATH=\"nlabel_cf_train.tsv\"\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_bimpm/${RESULT_DIR}/predictions_train.json label_predictor_bimpm/${RESULT_DIR}/model.tar.gz label_predictor_bimpm/${TEST_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Using wandb for parameters adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/wandb_label_predictor1.yaml\n",
    "\n",
    "name: label_predictor_stacked\n",
    "program: wandb_allennlp # this is a wrapper console script around allennlp commands. It is part of wandb-allennlp\n",
    "method: bayes\n",
    "## Do not for get to use the command keyword to specify the following command structure\n",
    "command:\n",
    "  - ${program} #omit the interpreter as we use allennlp train command directly\n",
    "  - \"--subcommand=train\"\n",
    "  - \"--include-package=bimpm_custom_package\" # add all packages containing your registered classes here\n",
    "  - \"--config_file=label_predictor_bimpm/config_elmo.json\"\n",
    "  - ${args}\n",
    "metric:\n",
    "    name: best_f1_macro\n",
    "    goal: maximize\n",
    "parameters:\n",
    "    iterator.batch_size:\n",
    "        values: [4,]\n",
    "    model.encode_together:\n",
    "        values: [\"true\", ]\n",
    "    trainer.optimizer.lr:\n",
    "        values: [0.001,]\n",
    "    model.dropout:\n",
    "        values: [0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/wandb_label_predictor2.yaml\n",
    "\n",
    "name: label_predictor_stacked\n",
    "program: wandb_allennlp # this is a wrapper console script around allennlp commands. It is part of wandb-allennlp\n",
    "method: bayes\n",
    "## Do not for get to use the command keyword to specify the following command structure\n",
    "command:\n",
    "  - ${program} #omit the interpreter as we use allennlp train command directly\n",
    "  - \"--subcommand=train\"\n",
    "  - \"--include-package=bimpm_custom_package\" # add all packages containing your registered classes here\n",
    "  - \"--config_file=label_predictor_bimpm/config_elmo_fasttext.json\"\n",
    "  - ${args}\n",
    "metric:\n",
    "    name: best_f1_macro\n",
    "    goal: maximize\n",
    "parameters:\n",
    "    iterator.batch_size:\n",
    "        values: [4,]\n",
    "    model.encode_together:\n",
    "        values: [\"true\", ]\n",
    "    trainer.optimizer.lr:\n",
    "        values: [0.001,]\n",
    "    model.dropout:\n",
    "        values: [0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ../../../maintenance_rst/models/label_predictor_bimpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/label_predictor_bimpm ../../../maintenance_rst/models/label_predictor_bimpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``wandb sweep wandb_label_predictor1.yaml``\n",
    "\n",
    "(returns %sweepname1)\n",
    "\n",
    "``wandb sweep wandb_label_predictor2.yaml``\n",
    "\n",
    "(returns %sweepname2)\n",
    "\n",
    "``wandb agent --count 1 %sweepname1 && wandb agent --count 1 %sweepname2``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the best model in label_predictor_bimpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls -laht models/wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/wandb/run-20200721_172146-kggsduvw/training_dumps models/label_predictor_bimpm/noble-sweep-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or** load from wandb by %sweepname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "run = api.run(\"tchewik/tmp/7hum4oom\")\n",
    "for file in run.files():\n",
    "    file.download(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r training_dumps models/label_predictor_bimpm/toasty-sweep-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run evaluation from shell\n",
    "\n",
    "``sh eval_label_predictor.sh {elmo|elmo_fasttext} toasty-sweep-1``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            result.append(json.loads(line)[\"label\"])\n",
    "            \n",
    "    print('length of result:', len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT_DIR = 'noble-sweep-3'  # ELMo\n",
    "# RESULT_DIR = 'polar-sweep-3'  # ELMo + fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = 'winter-sweep-1'  # ELMo\n",
    "# RESULT_DIR = 'twilight-sweep-1'  # ELMo + fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/label_predictor_bimpm/$RESULT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../../../maintenance_rst/models/label_predictor_bimpm/$RESULT_DIR/* models/label_predictor_bimpm/$RESULT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r ../../../maintenance_rst/models/label_predictor_bimpm/$RESULT_DIR/vocabulary models/label_predictor_bimpm/$RESULT_DIR/vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(DEV_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = classification_report(true[:len(pred)], pred, digits=4, output_dict=True)\n",
    "test_f1 = np.array(\n",
    "    [test_metrics[label].get('f1-score') for label in test_metrics if type(test_metrics[label]) == dict]) * 100\n",
    "\n",
    "test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred, average='macro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_confusion_matrix import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = list(set(true))\n",
    "labels.sort()\n",
    "plot_confusion_matrix(confusion_matrix(true[:len(pred)], pred, labels), target_names=labels, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_classes = [\n",
    "    'attribution_NS',\n",
    "    'attribution_SN',\n",
    "    'purpose_NS',\n",
    "    'purpose_SN',\n",
    "    'condition_SN',\n",
    "    'contrast_NN',\n",
    "    'condition_NS',\n",
    "    'joint_NN',\n",
    "    'concession_NS',\n",
    "    'same-unit_NN',\n",
    "    'elaboration_NS',\n",
    "    'cause-effect_NS',\n",
    "]\n",
    "\n",
    "class_mapper = {weird_class: 'other' + weird_class[-3:] for weird_class in labels if not weird_class in top_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "true = [class_mapper.get(value) if class_mapper.get(value) else value for value in true]\n",
    "pred = [class_mapper.get(value) if class_mapper.get(value) else value for value in pred]\n",
    "\n",
    "pred_mapper = {\n",
    "    'other_NN': 'joint_NN',\n",
    "    'other_NS': 'joint_NN',\n",
    "    'other_SN': 'joint_NN'\n",
    "}\n",
    "pred = [pred_mapper.get(value) if pred_mapper.get(value) else value for value in pred]\n",
    "\n",
    "#pred = [value if not 'other' in value else true[i] for i, value in enumerate(pred)]\n",
    "\n",
    "_to_stay = (np.array(true) != 'other_NN') & (np.array(true) != 'other_SN') & (np.array(true) != 'other_NS')\n",
    "\n",
    "_true = np.array(true)[_to_stay]\n",
    "_pred = np.array(pred)[_to_stay[:len(pred)]]\n",
    "labels = list(set(_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred, average='macro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(_true[:len(_pred)], _pred), target_names=labels, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for rel in np.unique(_true):\n",
    "    print(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On train set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv('models/label_predictor_bimpm/nlabel_cf_train.tsv', sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_train.json')\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = 'models/label_predictor_lstm/nlabel_cf_train.tsv'\n",
    "true_train = pd.read_csv(file, sep='\\t', header=None)\n",
    "true_train['predicted_relation'] = pred\n",
    "\n",
    "print(true_train[true_train.relation != true_train.predicted_relation].shape)\n",
    "\n",
    "true_train[true_train.relation != true_train.predicted_relation].to_csv('mispredicted_relations.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(TEST_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = classification_report(true[:len(pred)], pred, digits=4, output_dict=True)\n",
    "test_f1 = np.array(\n",
    "    [test_metrics[label].get('f1-score') for label in test_metrics if type(test_metrics[label]) == dict]) * 100\n",
    "\n",
    "test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred, average='macro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = [class_mapper.get(value) if class_mapper.get(value) else value for value in true]\n",
    "pred = [class_mapper.get(value) if class_mapper.get(value) else value for value in pred]\n",
    "pred = [pred_mapper.get(value) if pred_mapper.get(value) else value for value in pred]\n",
    "\n",
    "_to_stay = (np.array(true) != 'other_NN') & (np.array(true) != 'other_SN') & (np.array(true) != 'other_NS')\n",
    "\n",
    "_true = np.array(true)[_to_stay]\n",
    "_pred = np.array(pred)[_to_stay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(_true[:len(_pred)], _pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(_true[:len(_pred)], _pred, average='macro')*100))\n",
    "print('pr: %.2f'%(precision_score(_true[:len(_pred)], _pred, average='macro')*100))\n",
    "print('re: %.2f'%(recall_score(_true[:len(_pred)], _pred, average='macro')*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: (Logreg+Catboost) + BiMPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab = [\n",
    "    \"joint_NN\",\n",
    "    \"elaboration_NS\",\n",
    "    \"contrast_NN\",\n",
    "    \"attribution_SN\",\n",
    "    \"interpretation-evaluation_NS\",\n",
    "    \"cause-effect_SN\",\n",
    "    \"preparation_SN\",\n",
    "    \"sequence_NN\",\n",
    "    \"cause-effect_NS\",\n",
    "    \"same-unit_NN\",\n",
    "    \"condition_SN\",\n",
    "    \"purpose_NS\",\n",
    "    \"attribution_NS\",\n",
    "    \"condition_NS\",\n",
    "    \"comparison_NN\",\n",
    "    \"background_NS\",\n",
    "    \"evidence_NS\",\n",
    "    \"solutionhood_SN\",\n",
    "    \"concession_NS\",\n",
    "    \"interpretation-evaluation_SN\",\n",
    "    \"restatement_NN\",\n",
    "    \"purpose_SN\",\n",
    "]\n",
    "\n",
    "catboost_vocab = [\n",
    "   'attribution_NS', 'attribution_SN', 'background_NS',\n",
    "   'cause-effect_NS', 'cause-effect_SN', 'comparison_NN',\n",
    "   'concession_NS', 'condition_NS', 'condition_SN', 'contrast_NN',\n",
    "   'elaboration_NS', 'evidence_NS', 'interpretation-evaluation_NS',\n",
    "   'interpretation-evaluation_SN', 'joint_NN', 'preparation_SN',\n",
    "   'purpose_NS', 'purpose_SN', 'restatement_NN', 'same-unit_NN',\n",
    "   'sequence_NN', 'solutionhood_SN']\n",
    "\n",
    "def load_neural_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            probs = json.loads(line)['probs']\n",
    "            probs = {model_vocab[i]: probs[i] for i in range(len(model_vocab))}\n",
    "            result.append(probs)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def load_scikit_predictions(model, X):\n",
    "    result = []\n",
    "    predictions = model.predict_proba(X)\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        probs = {catboost_vocab[j]: prediction[j] for j in range(len(catboost_vocab))}\n",
    "        result.append(probs)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vote_predictions(pred1, pred2, soft=True, weights=[1., 1.]):\n",
    "    assert len(pred1) == len(pred2)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(pred1)):\n",
    "        sample_result = {}\n",
    "        for key in pred1[i].keys():\n",
    "            if soft:\n",
    "                sample_result[key] = pred1[i][key]*weights[0] + pred2[i][key]*weights[1]\n",
    "            else:\n",
    "                sample_result[key] = max(pred1[i][key], pred2[i][key])\n",
    "        \n",
    "        result.append(sample_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def probs_to_classes(pred):\n",
    "    result = []\n",
    "    \n",
    "    for sample in pred:\n",
    "        best_class = ''\n",
    "        best_prob = 0.\n",
    "        for key in sample.keys():\n",
    "            if sample[key] > best_prob:\n",
    "                best_prob = sample[key]\n",
    "                best_class = key\n",
    "        \n",
    "        result.append(best_class)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fs_catboost_plus_logreg = pickle.load(open('models/relation_predictor_baseline/model.pkl', 'rb'))\n",
    "lab_encoder = pickle.load(open('models/relation_predictor_baseline/label_encoder.pkl', 'rb'))\n",
    "scaler = pickle.load(open('models/relation_predictor_baseline/scaler.pkl', 'rb'))\n",
    "drop_columns = pickle.load(open('models/relation_predictor_baseline/drop_columns.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "TARGET = 'relation'\n",
    "\n",
    "y_dev, X_dev = dev_samples['relation'].to_frame(), dev_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id', 'index'])\n",
    "\n",
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)\n",
    "\n",
    "catboost_predictions = load_scikit_predictions(fs_catboost_plus_logreg, X_dev)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, catboost_predictions, soft=True, weights=[1., 2.])\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_dev.values, ensemble_pred, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_dev.values, ensemble_pred, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_dev.values, ensemble_pred))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'relation'\n",
    "\n",
    "y_test, X_test = test_samples[TARGET].to_frame(), test_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id', 'index'])\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)\n",
    "\n",
    "catboost_predictions = load_scikit_predictions(fs_catboost_plus_logreg, X_test)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, catboost_predictions, soft=True, weights=[1., 2.])\n",
    "\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_test.values, ensemble_pred, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_test.values, ensemble_pred, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_test.values, ensemble_pred))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = classification_report(y_test, ensemble_pred, digits=4, output_dict=True)\n",
    "test_f1 = np.array(\n",
    "    [test_metrics[label].get('f1-score') for label in test_metrics if type(test_metrics[label]) == dict]) * 100\n",
    "\n",
    "test_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
