{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse RS3 files \n",
    "output:\n",
    " - ``data/file.edus``  - text file with edus from .rs3 - each line contains one edu\n",
    " - ``data/file.json``  - json file with du-pairs from gold trees. keys: ``['snippet_x', 'snippet_y', 'category_id']``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> in the original RuRSTreebank dataset, some deprecated symbols occure (>, <, &, etc.), breaking the xml parser, as well as EDUs with punctuation marks at the beginning (it happens when brackets and dots/commas are separated with space in the original text). The latest version of the corpus (at the time of this notebooks' latest commit) has been corrected and dumped in <b>corpus/RuRsTreebank_full_corrected.zip</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd corpus/\n",
    "unzip RuRsTreebank_full_v6_corrected.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir data\n",
    "python utils/parse_rs3.py corpus/RuRsTreebank_full_6/blogs/blogs_rs3/* > rst_blogs_parsing.log\n",
    "python utils/parse_rs3.py corpus/RuRsTreebank_full_6/news1/news1_rs3/* > rst_news1_parsing.log\n",
    "python utils/parse_rs3.py corpus/RuRsTreebank_full_6/news2/news2_rs3/* > rst_news2_parsing.log\n",
    "\n",
    "#python utils/parse_rs3.py corpus/RuRsTreebank_full_5/sci_comp/sci_comp_rs3/* > rst_scicomp_parsing.log\n",
    "#python utils/parse_rs3.py corpus/RuRsTreebank_full_5/sci_ling/sci_ling_rs3/* > rst_sciling_parsing.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the texts with isanlp \n",
    "output:\n",
    " - file.annot.pkl  # morphology, syntax, semantics to use with isanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U git+https://github.com/IINemo/isanlp.git@discourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "\n",
    "host_udpipe = ''\n",
    "\n",
    "ppl = PipelineCommon([\n",
    "    (ProcessorRemote(host_udpipe, 3344, '0'),\n",
    "     ['text'],\n",
    "     {'sentences': 'sentences',\n",
    "      'tokens': 'tokens',\n",
    "      'lemma': 'lemma',\n",
    "      'syntax_dep_tree': 'syntax_dep_tree',\n",
    "      'postag': 'ud_postag'}),\n",
    "    (ProcessorMystem(delay_init=False),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'postag': 'postag'}),\n",
    "    (ConverterMystemToUd(),\n",
    "     ['postag'],\n",
    "     {'morph': 'morph',\n",
    "      'postag': 'postag'}),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import _prepare_text as prepare_text\n",
    "\n",
    "directories = ['corpus/RuRsTreebank_full_6/blogs/blogs_txt/',\n",
    "               'corpus/RuRsTreebank_full_6/news1/news1_txt/',\n",
    "               'corpus/RuRsTreebank_full_6/news2/news2_txt/'\n",
    "               ]\n",
    "\n",
    "for path in directories:\n",
    "    print('analyze path:', path)\n",
    "    for file in tqdm(glob.glob(f'{path}*.txt')):\n",
    "        text = prepare_text(open(file, 'r').read())\n",
    "        annot = ppl(text)\n",
    "        filename = file.split('/')[-1].replace('.txt', '.annot.pkl')\n",
    "        pickle.dump(annot, open(os.path.join('data', filename), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) parse science texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import _prepare_text as prepare_text\n",
    "\n",
    "directories = ['corpus/RuRsTreebank_full_6/sci_comp/sci_comp_txt/',\n",
    "               'corpus/RuRsTreebank_full_6/sci_ling/sci_ling_txt/',\n",
    "               ]\n",
    "\n",
    "for path in directories:\n",
    "    print('analyze path:', path)\n",
    "    for file in tqdm(glob.glob(f'{path}*.txt')):\n",
    "        text = open(file, 'r').read()\n",
    "        text = text.replace('  \\n', '#####').replace('\\n', ' ')\n",
    "        text = prepare_text(text)\n",
    "        annot = ppl(text)\n",
    "        filename = file.split('/')[-1].replace('.txt', '.annot.pkl')\n",
    "        pickle.dump(annot, open(os.path.join('data', filename), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold trees\n",
    "### Extract features \n",
    "output:\n",
    " - models/tf_idf/pipeline.pkl  # is used in default feature extraction\n",
    " - file.gold.pkl  # dataset with extracted default features for gold trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from utils.file_reading import read_annotation\n",
    "\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "! mkdir models\n",
    "! mkdir models/tf_idf\n",
    "\n",
    "corpus = []\n",
    "for file in glob.glob(\"%s*.json\" % IN_PATH):\n",
    "    tokens = read_annotation(file.replace('.json', ''))['tokens']\n",
    "    corpus.append(list(map(lambda token: token.text.lower(), tokens)))\n",
    "\n",
    "    \n",
    "from utils.count_vectorizer import MyCountVectorizer\n",
    "count_vect = MyCountVectorizer(ngram_range=(1, 2), tokenizer=MyCountVectorizer.dummy, preprocessor=MyCountVectorizer.dummy)\n",
    "\n",
    "svd = TruncatedSVD(n_components=25,\n",
    "                   tol=0.0,\n",
    "                   n_iter=7,\n",
    "                   random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('svd', svd)\n",
    "])\n",
    "\n",
    "pipeline.fit(corpus)\n",
    "pickle.dump(pipeline, open('models/tf_idf/pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -c \"import nltk; nltk.download('stopwords')\"\n",
    "pip install dostoevsky\n",
    "dostoevsky download fasttext-social-network-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../isanlp_rst/utils/features_processor_variables.py utils/features_processor_variables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"scikit_learn==0.22.2.post1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "from isanlp_rst.src.isanlp_rst.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_gold, read_annotation\n",
    "\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    table = read_gold(file.replace('.json', ''))\n",
    "    table = table[table.snippet_x.map(len) > 0]\n",
    "    table = table[table.snippet_y.map(len) > 0]\n",
    "    annot = read_annotation(file.replace('.json', ''))\n",
    "    features = features_processor(table, \n",
    "                                  annot['text'], annot['tokens'], \n",
    "                                  annot['sentences'], annot['lemma'], \n",
    "                                  annot['morph'], annot['ud_postag'], \n",
    "                                  annot['syntax_dep_tree'])\n",
    "    features.to_pickle(file.replace('.json', '.gold.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
