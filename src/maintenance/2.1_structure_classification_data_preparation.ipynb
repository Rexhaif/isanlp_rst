{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building: Step 1. Negative samples generation\n",
    "\n",
    "Create train and test sets; Save negative samples of file ``filename.rs3`` as `filename.neg`\n",
    "\n",
    "Output:\n",
    " - ``data/*.neg``\n",
    " - ``data_structure/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "from isanlp_rst.src.isanlp_rst.rst_tree_predictor import RSTTreePredictor, GoldTreePredictor\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "from utils.file_reading import *\n",
    "from utils.print_tree import printBTree\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNegativeGenerator(object):\n",
    "    def __call__(self, edus, corpus, annot_text):\n",
    "        new_set = self.create_training_set(edus, corpus)\n",
    "        result = []\n",
    "        for item in new_set:\n",
    "            result.append((filename, item[0], item[1], item[2]))\n",
    "\n",
    "        tmp = pd.DataFrame(result, columns=['filename', 'snippet_x', 'snippet_y', 'relation'])\n",
    "\n",
    "        def place_locations(row):\n",
    "            row['loc_x'] = annot_text.find(row.snippet_x)\n",
    "            row['loc_y'] = annot_text.find(row.snippet_y, row['loc_x'] + 1)\n",
    "            return row\n",
    "\n",
    "        return tmp.apply(place_locations, axis=1)\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'RandomNegativeGenerator'\n",
    "    \n",
    "    def create_training_set(self, edus, gold):\n",
    "        training_set = []\n",
    "        \n",
    "        snippet_cache = []\n",
    "        for num, e in enumerate(gold.index):\n",
    "            snippet_x = gold.loc[e, 'snippet_x']\n",
    "            cache_x = self.extract_snippet_ids(snippet_x, edus)\n",
    "\n",
    "            snippet_y = gold.loc[e, 'snippet_y']\n",
    "            cache_y = self.extract_snippet_ids(snippet_y, edus)\n",
    "\n",
    "            if cache_x and cache_y:\n",
    "                snippet_cache.append((cache_x, snippet_x))\n",
    "                snippet_cache.append((cache_y, snippet_y))\n",
    "\n",
    "        for i in range(len(edus) - 1):\n",
    "            if not self.check_snippet_pair_in_dataset(gold, edus[i], edus[i+1]):\n",
    "                training_set.append((edus[i], edus[i+1], False))\n",
    "\n",
    "        for i in gold.index:\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_x'])\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_y'])\n",
    "\n",
    "        for i in range(len(snippet_cache)):\n",
    "            for j in range(i, len(snippet_cache)):\n",
    "                cache_i, snippet_i = snippet_cache[i]\n",
    "                cache_j, snippet_j = snippet_cache[j]\n",
    "\n",
    "                if cache_i[-1] + 1 == cache_j[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_i, snippet_j):\n",
    "                        training_set.append((snippet_i, snippet_j, False))\n",
    "\n",
    "                if cache_j[-1] + 1 == cache_i[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_j, snippet_i):\n",
    "                        training_set.append((snippet_j, snippet_i, False))\n",
    "\n",
    "        return list(set(training_set))\n",
    "    \n",
    "    def extract_snippet_ids(self, snippet, edus):\n",
    "        return [edu_nm for edu_nm, edu in enumerate(edus) if (edu in snippet)]\n",
    "    \n",
    "    def check_snippet_pair_in_dataset(self, dataset, snippet_left, snippet_right):\n",
    "        return ((((dataset.snippet_x == snippet_left) & (dataset.snippet_y == snippet_right)).sum(axis=0) != 0) \n",
    "                or ((dataset.snippet_y == snippet_left) & (dataset.snippet_x == snippet_right)).sum(axis=0) != 0)\n",
    "    \n",
    "    def extract_negative_samples_for_snippet(self, gold, edus, snippet):\n",
    "        training_set = []\n",
    "\n",
    "        snippet_ids = self.extract_snippet_ids(snippet, edus)\n",
    "\n",
    "        if not snippet_ids:\n",
    "            return []\n",
    "\n",
    "        if snippet_ids[0] > 0:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[0] - 1]):\n",
    "                training_set.append((edus[snippet_ids[0] - 1], snippet, False))\n",
    "\n",
    "        if snippet_ids[-1] < len(edus) - 1:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[-1] + 1]):\n",
    "                training_set.append((snippet, edus[snippet_ids[-1] + 1], False))\n",
    "\n",
    "        return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GreedyNegativeGenerator:\n",
    "    \"\"\" Inversed greedy parser based on gold tree predictor. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.forest_threshold = 0.01\n",
    "        self._same_sentence_bonus = 0\n",
    "\n",
    "    def __call__(self, edus, corpus,\n",
    "                 annot_text, annot_tokens,\n",
    "                 annot_sentences,\n",
    "                 annot_lemma, annot_morph, annot_postag,\n",
    "                 annot_syntax_dep_tree):\n",
    "\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "\n",
    "        negative_nodes = []\n",
    "\n",
    "        self.tree_predictor = GoldTreePredictor(corpus)\n",
    "        nodes = edus\n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = self.tree_predictor.initialize_features(nodes,\n",
    "                                                           annot_text, annot_tokens,\n",
    "                                                           annot_sentences,\n",
    "                                                           annot_lemma, annot_morph, annot_postag,\n",
    "                                                           annot_syntax_dep_tree)\n",
    "\n",
    "        scores = self.tree_predictor.predict_pair_proba(features, _same_sentence_bonus=self._same_sentence_bonus)\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            if score == 0:\n",
    "                negative_nodes.append(\n",
    "                    DiscourseUnit(\n",
    "                        id=None,\n",
    "                        left=nodes[i],\n",
    "                        right=nodes[i + 1],\n",
    "                        relation='no_relation',\n",
    "                        nuclearity='NN',\n",
    "                        proba=score,\n",
    "                        text=annot_text[nodes[i].start:nodes[i + 1].end].strip()\n",
    "                    ))\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "\n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            relation = self.tree_predictor.predict_label(features.iloc[j])\n",
    "            relation, nuclearity = relation.split('_')\n",
    "\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=relation,\n",
    "                nuclearity=nuclearity,\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "            )\n",
    "\n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                scores = _scores + scores[j + 2:]\n",
    "                features = pd.concat([_features, features.iloc[j + 2:]])\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[0],\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                _features = self.tree_predictor.initialize_features([nodes[j - 1], nodes[j], nodes[j + 1]],\n",
    "                                                                    annot_text, annot_tokens,\n",
    "                                                                    annot_sentences,\n",
    "                                                                    annot_lemma, annot_morph, annot_postag,\n",
    "                                                                    annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                features = pd.concat([features.iloc[:j - 1], _features, features.iloc[j + 2:]])\n",
    "                scores = scores[:j - 1] + _scores + scores[j + 2:]\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[0],\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "                if _scores[1] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[1],\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            else:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                scores = scores[:j - 1] + _scores\n",
    "                features = pd.concat([features.iloc[:j - 1], _features])\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores,\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "        return negative_nodes\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'GreedyNegativeGenerator'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make negative samples, save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen1 = RandomNegativeGenerator()\n",
    "gen2 = GreedyNegativeGenerator()\n",
    "\n",
    "for filename in tqdm(glob.glob('./data/*.json')):\n",
    "    filename = filename.replace('.json', '')\n",
    "    df = read_gold(filename, features=True)\n",
    "    edus = read_edus(filename)\n",
    "    annot = read_annotation(filename)\n",
    "\n",
    "    tmp = gen1(edus, df, annot['text'])\n",
    "#     tmp1 = gen1(edus, df, annot['text'])\n",
    "    \n",
    "#     _edus = []\n",
    "#     last_end = 0\n",
    "#     last_id = 0\n",
    "#     for max_id in range(len(edus)):\n",
    "#         start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "#         end = start + len(edus[max_id])\n",
    "#         temp = DiscourseUnit(\n",
    "#             id=max_id + last_id,\n",
    "#             left=None,\n",
    "#             right=None,\n",
    "#             relation='edu',\n",
    "#             start=start,\n",
    "#             end=end,\n",
    "#             orig_text=annot['text'],\n",
    "#             proba=1.,\n",
    "#             )\n",
    "#         _edus.append(temp)\n",
    "#         last_end = end\n",
    "#         last_id += 1\n",
    "\n",
    "#     tmp = gen2(_edus, df, annot['text'],\n",
    "#                annot['tokens'], annot['sentences'],\n",
    "#                annot['lemma'], annot['morph'],\n",
    "#                annot['postag'], annot['syntax_dep_tree'])\n",
    "    \n",
    "#     tmp = pd.DataFrame(extr_pairs_forest(tmp, annot['text'], locations=True), \n",
    "#                        columns=['snippet_x', 'snippet_y', \n",
    "#                                 'category_id', 'order', \n",
    "#                                 'loc_x', 'loc_y'])\n",
    "    \n",
    "#     tmp = tmp[tmp.category_id == 'no_relation']\n",
    "#     tmp = tmp.drop(columns=['order', 'category_id'])\n",
    "#     tmp['filename'] = filename\n",
    "#     tmp['relation'] = False\n",
    "    \n",
    "#     tmp = pd.concat([tmp, tmp1])\n",
    "    tmp = tmp[(tmp.loc_x < tmp.loc_y) & (tmp.loc_x > -1)]\n",
    "    \n",
    "    tmp.drop_duplicates(['snippet_x', 'snippet_y']).reset_index().to_json(filename + '.json.neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from isanlp_rst.src.isanlp_rst.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "MAX_LEN = 100\n",
    "\n",
    "for filename in tqdm(glob.glob(\"data/*.json.neg\")):    \n",
    "    filename = filename.replace('.json.neg', '')\n",
    "    \n",
    "    df = read_negative(filename).drop(columns=['loc_y'])\n",
    "    df = df[df.snippet_x.str.len() > 0]\n",
    "    df = df[df.snippet_y.str.len() > 0]\n",
    "    \n",
    "    annotation = read_annotation(filename)\n",
    "        \n",
    "    result = features_processor(df,\n",
    "                                annotation['text'],\n",
    "                                annotation['tokens'],\n",
    "                                annotation['sentences'],\n",
    "                                annotation['lemma'],\n",
    "                                annotation['morph'],\n",
    "                                annotation['postag'],\n",
    "                                annotation['syntax_dep_tree'])\n",
    "    \n",
    "    result = result[result.is_broken == False]\n",
    "    \n",
    "    result = result[result.tokens_x.map(len) < MAX_LEN]\n",
    "    result = result[result.tokens_y.map(len) < MAX_LEN]\n",
    "    \n",
    "    result.to_pickle(filename + '.neg.features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train/test splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.file_reading import read_gold\n",
    "\n",
    "\n",
    "random_state = 45\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "dev_samples = []\n",
    "\n",
    "for file in tqdm(train):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    train_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    train_samples.append(negative)\n",
    "\n",
    "for file in tqdm(dev):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    dev_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    dev_samples.append(negative)\n",
    "    \n",
    "for file in tqdm(test):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    test_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    test_samples.append(negative)\n",
    "\n",
    "train_samples = pd.concat(train_samples)\n",
    "dev_samples = pd.concat(dev_samples)\n",
    "test_samples = pd.concat(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.prepare_sequence import _prepare_sequence\n",
    "\n",
    "\n",
    "def correct_samples(row):\n",
    "    if row.snippet_x[0] in (',', '.', '!', '?'):\n",
    "        row.snippet_x = row.snippet_x[1:].strip()\n",
    "    if row.snippet_y[0] in (',', '.'):\n",
    "        row.snippet_x += row.snippet_y[0]\n",
    "        row.snippet_y = row.snippet_y[1:].strip()\n",
    "    return row\n",
    "\n",
    "def prepare_data(data, max_len=100):\n",
    "\n",
    "    data = data[data.tokens_x.map(len) < max_len]\n",
    "    data = data[data.tokens_y.map(len) < max_len]\n",
    "    \n",
    "    data['snippet_x'] = data.tokens_x.map(lambda row: ' '.join(row))\n",
    "    data['snippet_y'] = data.tokens_y.map(lambda row: ' '.join(row))\n",
    "    \n",
    "    data = data.apply(correct_samples, axis=1)\n",
    "    \n",
    "    data = data[data.snippet_x.map(len) > 0]\n",
    "    data = data[data.snippet_y.map(len) > 0]\n",
    "    \n",
    "    data['snippet_x'] = data.snippet_x.map(_prepare_sequence)\n",
    "    data['snippet_y'] = data.snippet_y.map(_prepare_sequence)\n",
    "    \n",
    "    data = data.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last')\n",
    "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "train_samples = prepare_data(train_samples)\n",
    "dev_samples = prepare_data(dev_samples)\n",
    "test_samples = prepare_data(test_samples)\n",
    "\n",
    "OUT_PATH = 'data_structure'\n",
    "! mkdir $OUT_PATH\n",
    "train_samples.to_pickle(os.path.join(OUT_PATH, 'train_samples.pkl'))\n",
    "dev_samples.to_pickle(os.path.join(OUT_PATH, 'dev_samples.pkl'))\n",
    "test_samples.to_pickle(os.path.join(OUT_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples[['snippet_x', 'snippet_y', 'relation', 'filename']].sort_values('snippet_x').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples['len_x'] = train_samples.snippet_x.map(lambda row: len(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
