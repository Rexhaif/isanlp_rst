{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER0 = ''\n",
    "SERVER2 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "\n",
    "address_morph = (SERVER0, 4333)\n",
    "address_syntax = (SERVER2, 3334)\n",
    "address_rst = (SERVER0, 3346)\n",
    "address_rst = (SERVER2, 3345)\n",
    "\n",
    "ppl = PipelineCommon([\n",
    "    (ProcessorRemote(address_syntax[0], address_syntax[1], '0'),\n",
    "     ['text'],\n",
    "     {'sentences': 'sentences',\n",
    "      'tokens': 'tokens',\n",
    "      'lemma': 'lemma',\n",
    "      'syntax_dep_tree': 'syntax_dep_tree',\n",
    "      'postag': 'ud_postag'}),\n",
    "    (ProcessorMystem(delay_init=False),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'postag': 'postag'}),\n",
    "    (ConverterMystemToUd(),\n",
    "     ['postag'],\n",
    "     {'morph': 'morph',\n",
    "      'postag': 'postag'}),\n",
    "    (ProcessorRemote(address_rst[0], address_rst[1], 'default'),\n",
    "     ['text', 'tokens', 'sentences', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "     {'rst': 'rst'})\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_annotation, read_edus, read_gold\n",
    "from utils.evaluation import *\n",
    "\n",
    "example = 'data/news2_4'\n",
    "text = open('corpus/RuRsTreebank_full_2/news2/news2_txt/news2_4.txt', 'r').read().strip()\n",
    "gold_edus = read_edus(example)\n",
    "gold_pairs = prepare_gold_pairs(read_gold(example, features=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = ppl(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in result['rst']:\n",
    "    print(tree.proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ex = ForestExporter()\n",
    "ex(result['rst'], 'news2_4_pred.rs3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import *\n",
    "\n",
    "pred_edus = []\n",
    "for tree in result['rst']:\n",
    "    pred_edus += extr_edus(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gold_edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_edus), len(gold_edus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/evaluation.py\n",
    "\n",
    "import pandas as pd\n",
    "from utils.file_reading import text_html_map\n",
    "\n",
    "labels = ['condition_NS',\n",
    "     'concession_NS',\n",
    "     'elaboration_NS',\n",
    "     'preparation_SN',\n",
    "     #'background_SN',\n",
    "     'condition_SN',\n",
    "     'purpose_NS',\n",
    "     'cause-effect_NS',\n",
    "     'background_NS',\n",
    "     'interpretation-evaluation_NS',\n",
    "     'evidence_NS',\n",
    "     'same-unit_NN',\n",
    "     'joint_NN',\n",
    "     'attribution_SN',\n",
    "     'contrast_NN',\n",
    "     'restatement_NN',\n",
    "     'comparison_NN',\n",
    "     'cause-effect_SN',\n",
    "     'solutionhood_SN',\n",
    "     'purpose_SN',\n",
    "     'sequence_NN',\n",
    "     'attribution_NS',\n",
    "     'interpretation-evaluation_SN']\n",
    "\n",
    "top_classes = [\n",
    "    'attribution_NS',\n",
    "    'attribution_SN',\n",
    "    'purpose_NS',\n",
    "    'purpose_SN',\n",
    "    'condition_SN',\n",
    "    'contrast_NN',\n",
    "    'condition_NS',\n",
    "    'joint_NN',\n",
    "    'concession_NS',\n",
    "    'same-unit_NN',\n",
    "    'elaboration_NS',\n",
    "    'cause-effect_NS',\n",
    "    #'solutionhood_SN',\n",
    "    #'cause-effect_SN'\n",
    "]\n",
    "\n",
    "class_mapper = {weird_class: 'other' + weird_class[-3:] for weird_class in labels if not weird_class in top_classes}\n",
    "\n",
    "pred_mapper = {\n",
    "    'other_NN': 'joint_NN',\n",
    "    'other_NS': 'joint_NN',\n",
    "    'other_SN': 'joint_NN'\n",
    "}\n",
    "\n",
    "target_map = {\n",
    "    'relation': 'joint',\n",
    "    'antithesis': 'contrast',\n",
    "    'cause': 'cause-effect',\n",
    "    'effect': 'cause-effect',\n",
    "    'conclusion': 'restatement',\n",
    "    'interpretation': 'interpretation-evaluation',\n",
    "    'evaluation': 'interpretation-evaluation',\n",
    "    'motivation': 'condition',\n",
    "}\n",
    "\n",
    "relation_map = {\n",
    "    'restatement_SN': 'restatement_NN',\n",
    "    'restatement_NS': 'restatement_NN',\n",
    "    'contrast_SN': 'contrast_NN',\n",
    "    'contrast_NS': 'contrast_NN',\n",
    "    'solutionhood_NS': 'elaboration_NS',\n",
    "    'preparation_NS': 'elaboration_NS',\n",
    "    'concession_SN': 'preparation_SN',\n",
    "    'evaluation_SN': 'preparation_SN',\n",
    "    'elaboration_SN': 'preparation_SN',\n",
    "    'evidence_SN': 'preparation_SN',\n",
    "    'background_SN': 'preparation_SN'\n",
    "}\n",
    "\n",
    "def prepare_gold_pairs(gold_pairs):\n",
    "    TARGET = 'category_id'\n",
    "\n",
    "    gold_pairs['category_id'] = gold_pairs['category_id'].map(lambda row: row.split('_')[0])\n",
    "    gold_pairs['category_id'] = gold_pairs['category_id'].replace([0.0], 'same-unit')\n",
    "    gold_pairs['order'] = gold_pairs['order'].replace([0.0], 'NN')\n",
    "    gold_pairs['category_id'] = gold_pairs['category_id'].replace(target_map, regex=False)\n",
    "\n",
    "    gold_pairs['relation'] = gold_pairs['category_id'].map(lambda row: row) + '_' + gold_pairs['order']\n",
    "    gold_pairs['relation'] = gold_pairs['relation'].replace(relation_map, regex=False)\n",
    "    \n",
    "    for key, value in class_mapper.items():\n",
    "        gold_pairs['relation'] = gold_pairs['relation'].replace(key, value)\n",
    "        \n",
    "    gold_pairs['order'] = gold_pairs['relation'].map(lambda row: row.split('_')[1])\n",
    "    gold_pairs[TARGET] = gold_pairs['relation'].map(lambda row: row.split('_')[0])\n",
    "        \n",
    "    return gold_pairs\n",
    "\n",
    "def prepare_string(string):\n",
    "    for key, value in text_html_map.items():\n",
    "        string = string.replace(key, value).strip()\n",
    "                \n",
    "    if '-' in string:\n",
    "        string = string.replace('-', ' ').strip()\n",
    "\n",
    "    while '  ' in string:\n",
    "        string = string.replace('  ', ' ')\n",
    "        \n",
    "    return string.strip()\n",
    "\n",
    "def metric_parseval(parsed_pairs, gold, span=True, labeled=False, nuc=False):\n",
    "    \n",
    "    parsed_strings = []\n",
    "    for i in parsed_pairs.index:\n",
    "        if span:\n",
    "            x, y = prepare_string(parsed_pairs.loc[i, 'snippet_x']), prepare_string(parsed_pairs.loc[i, 'snippet_y'])\n",
    "\n",
    "        else:\n",
    "            x, y = '', ''\n",
    "            \n",
    "        label = parsed_pairs.loc[i, 'category_id'].split('_')[0]\n",
    "        nuclearity = parsed_pairs.loc[i, 'order']\n",
    "        merged_label = '_'.join([label, nuclearity])\n",
    "        \n",
    "        if labeled or nuc:\n",
    "            replacement_cand = class_mapper.get(merged_label)\n",
    "            if replacement_cand:\n",
    "                if 'other' in replacement_cand:\n",
    "                    label, nuclearity = pred_mapper.get(replacement_cand).split('_')\n",
    "                else:\n",
    "                    label, nuclearity = replacement_cand.split('_')\n",
    "            \n",
    "        label = label if labeled else ''\n",
    "        nuclearity = nuclearity if nuc else ''\n",
    "        \n",
    "        result = '&'.join([x, y, label, nuclearity])\n",
    "        parsed_strings.append(result)\n",
    "\n",
    "    parsed_strings = list(set(parsed_strings))\n",
    "\n",
    "    gold_strings = []\n",
    "    for i in gold.index:\n",
    "        if span:\n",
    "            x, y = prepare_string(gold.loc[i, 'snippet_x']), prepare_string(gold.loc[i, 'snippet_y'])\n",
    "\n",
    "        else:\n",
    "            x, y = '', ''\n",
    "\n",
    "        label = gold.loc[i, 'category_id'].split('_')[0] if labeled else ''\n",
    "        nuclearity = gold.loc[i, 'order'] if nuc else ''\n",
    "        merged_label = '_'.join([label, nuclearity])\n",
    "        \n",
    "        if labeled or nuc:\n",
    "            if class_mapper.get(merged_label):\n",
    "                label = class_mapper.get(merged_label).split('_')[0] if labeled else ''\n",
    "                nuclearity = class_mapper.get(merged_label).split('_')[1] if nuc else ''\n",
    "            \n",
    "        result = '&'.join([x, y, label, nuclearity])\n",
    "        gold_strings.append(result)\n",
    "\n",
    "    gold_strings = set(gold_strings)\n",
    "    \n",
    "    _to_exclude = [string.split('other')[0] for string in gold_strings if 'other' in string]\n",
    "    gold_strings = set([string for string in gold_strings if not 'other' in string])\n",
    "    \n",
    "    _remove_from_parsed_strings = []\n",
    "    for i, parsed_string in enumerate(parsed_strings):\n",
    "        for excluding_pair in _to_exclude:\n",
    "            if excluding_pair in parsed_string:\n",
    "                _remove_from_parsed_strings.append(i)\n",
    "    \n",
    "    parsed_strings = set([parsed_strings[i] for i in range(len(parsed_strings)) if not i in _remove_from_parsed_strings])\n",
    "\n",
    "    true_pos = len(gold_strings & parsed_strings)\n",
    "        \n",
    "    all_parsed = len(parsed_strings)\n",
    "    all_gold = len(gold_strings)\n",
    "    \n",
    "    return true_pos, all_parsed, all_gold\n",
    "\n",
    "\n",
    "def metric_parseval_df(parsed_pairs, gold, span=True, labeled=False, nuc=False):\n",
    "    parsed_strings = []\n",
    "\n",
    "    for i in parsed_pairs.index:\n",
    "        if span:\n",
    "            x, y = prepare_string(parsed_pairs.loc[i, 'snippet_x']), prepare_string(parsed_pairs.loc[i, 'snippet_y'])\n",
    "\n",
    "        else:\n",
    "            x, y = '', ''\n",
    "\n",
    "        label = ' ' + parsed_pairs.loc[i, 'category_id'].split('_')[0] if labeled else ''\n",
    "        nuclearity = ' ' + parsed_pairs.loc[i, 'order'] if nuc else ''\n",
    "        parsed_strings.append(x + ' ' + y + label + nuclearity)\n",
    "\n",
    "    parsed_strings = list(set(parsed_strings))\n",
    "\n",
    "    gold_strings = []\n",
    "    for i in gold.index:\n",
    "        if span:\n",
    "            x, y = prepare_string(gold.loc[i, 'snippet_x']), prepare_string(gold.loc[i, 'snippet_y'])\n",
    "\n",
    "        else:\n",
    "            x, y = '', ''\n",
    "\n",
    "        label = ' ' + gold.loc[i, 'category_id'].split('_')[0] if labeled else ''\n",
    "        nuclearity = ' ' + gold.loc[i, 'order'] if nuc else ''\n",
    "        gold_strings.append(x + ' ' + y + label + nuclearity)\n",
    "\n",
    "    gold_strings = set(gold_strings)\n",
    "    \n",
    "    _to_exclude = [string.split('other')[0] for string in gold_strings if 'other' in string]\n",
    "    gold_strings = set([string for string in gold_strings if not 'other' in string])\n",
    "    \n",
    "    _remove_from_parsed_strings = []\n",
    "    for i, parsed_string in enumerate(parsed_strings):\n",
    "        for excluding_pair in _to_exclude:\n",
    "            if excluding_pair in parsed_string:\n",
    "                _remove_from_parsed_strings.append(i)\n",
    "        \n",
    "    #all_parsed = [string for string in all_parsed if not 'other' in string]\n",
    "    parsed_strings = set([parsed_strings[i] for i in range(len(parsed_strings)) if not i in _remove_from_parsed_strings])\n",
    "\n",
    "    true_pos = len(gold_strings & parsed_strings)\n",
    "    all_parsed = len(parsed_strings)\n",
    "    all_gold = len(gold_strings)\n",
    "    return true_pos, all_parsed, all_gold\n",
    "\n",
    "\n",
    "def extr_pairs(tree):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text, tree.relation])\n",
    "        pp += extr_pairs(tree.left)\n",
    "        pp += extr_pairs(tree.right)\n",
    "    return pp\n",
    "\n",
    "\n",
    "def extr_pairs(tree, text, locations=False):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([text[tree.left.start:tree.left.end], text[tree.right.start:tree.right.end], tree.relation,\n",
    "                   tree.nuclearity] + [tree.left.start, tree.right.start] * locations)\n",
    "        pp += extr_pairs(tree.left, text, locations)\n",
    "        pp += extr_pairs(tree.right, text, locations)\n",
    "    return pp\n",
    "\n",
    "\n",
    "def extr_pairs_forest(forest, text, locations=False):\n",
    "    pp = []\n",
    "    for tree in forest:\n",
    "        pp += extr_pairs(tree, text, locations=locations)\n",
    "    return pp\n",
    "\n",
    "\n",
    "def _check_snippet_pair_in_dataset(left_snippet, right_snippet):\n",
    "    left_snippet = left_snippet.strip()\n",
    "    right_snippet = right_snippet.strip()\n",
    "    return ((((gold.snippet_x == left_snippet) & (gold.snippet_y == right_snippet)).sum(axis=0) != 0)\n",
    "            or ((gold.snippet_y == left_snippet) & (gold.snippet_x == right_snippet)).sum(axis=0) != 0)\n",
    "\n",
    "\n",
    "def _not_parsed_as_in_gold(parsed_pairs: pd.DataFrame, gold: pd.DataFrame, labeled=False):\n",
    "    for key in text_html_map.keys():\n",
    "        parsed_pairs['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        parsed_pairs['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    for key in text_html_map.keys():\n",
    "        gold['snippet_x'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "        gold['snippet_y'].replace(key, text_html_map[key], regex=True, inplace=True)\n",
    "\n",
    "    tmp = pd.merge(gold, parsed_pairs, on=['snippet_x', 'snippet_y'], how='left', suffixes=('_gold', '_parsed'))\n",
    "    if labeled:\n",
    "        tmp = tmp.fillna(0)\n",
    "        tmp = tmp[tmp.category_id_parsed != 0]\n",
    "        #tmp.category_id_gold = tmp.category_id_gold.map(lambda row: row[:-2])\n",
    "        return tmp[tmp.category_id_gold != tmp.category_id_parsed]\n",
    "    else:\n",
    "        return tmp[pd.isnull(tmp.category_id_parsed)]\n",
    "\n",
    "def extr_edus(tree):\n",
    "    edus = []\n",
    "    if tree.left:\n",
    "        edus += extr_edus(tree.left)\n",
    "        edus += extr_edus(tree.right)\n",
    "    else:\n",
    "        edus.append(tree.text)\n",
    "    return edus\n",
    "\n",
    "\n",
    "def eval_segmentation(trees, _gold_edus, verbose=False):\n",
    "    true_predictions = 0\n",
    "    all_predicted = 0\n",
    "    \n",
    "    gold_edus = []\n",
    "    \n",
    "    for gold_edu in _gold_edus:\n",
    "        gold_edus.append(prepare_string(gold_edu))\n",
    "\n",
    "    for tree in trees:\n",
    "        pred_edus = extr_edus(tree)\n",
    "        all_predicted += len(pred_edus)\n",
    "\n",
    "        for pred_edu in pred_edus:\n",
    "            pred_edu = prepare_string(pred_edu)\n",
    "\n",
    "            if prepare_string(pred_edu) in gold_edus:\n",
    "                true_predictions += 1\n",
    "                \n",
    "            elif verbose:\n",
    "                print(pred_edu)\n",
    "\n",
    "    return true_predictions, all_predicted, len(gold_edus)\n",
    "\n",
    "\n",
    "def eval_pipeline(trees=None, gold_edus=[], gold_pairs=pd.DataFrame([]), text=\"\", parsed_pairs=pd.DataFrame([])):\n",
    "    if parsed_pairs.empty:\n",
    "        parsed_pairs = extr_pairs_forest(trees, text)\n",
    "    \n",
    "    result = {}\n",
    "    result['seg_true_pred'], result['seg_all_pred'], result['seg_all_true'] = eval_segmentation(trees, gold_edus,\n",
    "                                                                                                verbose=False)\n",
    "    result['unlab_true_pred'], result['unlab_all_pred'], result['unlab_all_true'] = metric_parseval(parsed_pairs,\n",
    "                                                                                                    gold_pairs)\n",
    "    result['lab_true_pred'], result['lab_all_pred'], result['lab_all_true'] = metric_parseval(parsed_pairs, gold_pairs,\n",
    "                                                                                              labeled=True, nuc=False)\n",
    "    result['nuc_true_pred'], result['nuc_all_pred'], result['nuc_all_true'] = metric_parseval(parsed_pairs, gold_pairs,\n",
    "                                                                                              labeled=False, nuc=True)\n",
    "    result['full_true_pred'], result['full_all_pred'], result['full_all_true'] = metric_parseval(parsed_pairs, gold_pairs,\n",
    "                                                                                                labeled=True, nuc=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir parsing_results_0707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from utils.train_test_split import split_train_dev_test\n",
    "from utils.file_reading import read_edus, read_gold\n",
    "from utils.evaluation import *\n",
    "from utils.export_to_rs3 import ForestExporter\n",
    "import pickle\n",
    "\n",
    "pipeline_evaluation = {}\n",
    "train, dev, test = split_train_dev_test('data/')\n",
    "\n",
    "# news only\n",
    "#test = [filename for filename in test if 'news' in filename]\n",
    "cache = []\n",
    "thrown_error = []\n",
    "\n",
    "test.sort()\n",
    "ex = ForestExporter()\n",
    "\n",
    "for file in tqdm(test):\n",
    "    file = file.replace('.edus', '')\n",
    "    for name in ['news1', 'news2', 'blogs']:\n",
    "        if name in file:\n",
    "            text = open(f'corpus/RuRsTreebank_full/{name}/{file.replace(\"data/\", name+\"_txt/\")}.txt', 'r').read().strip()\n",
    "    if 'sci.ling' in file:\n",
    "        text = open(f'corpus/RuRsTreebank_full/sci_ling/sci_ling_txt/{file.replace(\"data/\", \"\")}.txt', 'r').read().strip()\n",
    "    elif 'sci.comp' in file:\n",
    "        text = open(f'corpus/RuRsTreebank_full/sci_comp/sci_comp_txt/{file.replace(\"data/\", \"\")}.txt', 'r').read().strip()\n",
    "\n",
    "    try:\n",
    "        result = ppl(text)\n",
    "        out_file = file.split('/')[-1]\n",
    "        pickle.dump(result, open(f'parsing_results_0707/{out_file}.pkl', 'wb'))\n",
    "\n",
    "        try:\n",
    "            ex(result['rst'], out_file+'_pred.rs3')\n",
    "        except:\n",
    "            print(out_file, \"was not saved in .rs3\")\n",
    "\n",
    "        gold_edus = read_edus(file)\n",
    "        gold_pairs = prepare_gold_pairs(read_gold(file, features=True))\n",
    "\n",
    "    #     evaluation = eval_pipeline(result['rst'], gold_edus, gold_pairs, result['text'])\n",
    "\n",
    "        parsed_pairs = pd.DataFrame(extr_pairs_forest(result['rst'], result['text']), \n",
    "                                    columns=['snippet_x', 'snippet_y', 'category_id', 'order'])\n",
    "\n",
    "        evaluation = eval_pipeline(parsed_pairs=parsed_pairs,\n",
    "                               gold_edus=gold_edus,\n",
    "                               gold_pairs=prepare_gold_pairs(read_gold(file, features=True)),\n",
    "                               text=result['text'],\n",
    "                               trees=result['rst'])\n",
    "\n",
    "        evaluation['filename'] = file\n",
    "        cache.append(evaluation)\n",
    "\n",
    "        #pipeline_evaluation[file] = eval_pipeline(result['rst'], gold_edus, gold_pairs)\n",
    "    except:\n",
    "        thrown_error.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_pairs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(gold_pairs) == int and gold_pairs == -1:\n",
    "    print([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(result, open(f'parsing_results_0707/{out_file}.pkl', 'wb'))\n",
    "    \n",
    "try:\n",
    "    ex(result['rst'], out_file+'_pred.rs3')\n",
    "except:\n",
    "    print(out_file, \"was not saved in .rs3\")\n",
    "\n",
    "gold_edus = read_edus(file)\n",
    "gold_pairs = prepare_gold_pairs(read_gold(file, features=True))\n",
    "\n",
    "#     evaluation = eval_pipeline(result['rst'], gold_edus, gold_pairs, result['text'])\n",
    "\n",
    "parsed_pairs = pd.DataFrame(extr_pairs_forest(result['rst'], result['text']), \n",
    "                            columns=['snippet_x', 'snippet_y', 'category_id', 'order'])\n",
    "\n",
    "evaluation = eval_pipeline(parsed_pairs=parsed_pairs,\n",
    "                       gold_edus=gold_edus,\n",
    "                       gold_pairs=prepare_gold_pairs(read_gold(file, features=True)),\n",
    "                       text=result['text'],\n",
    "                       trees=result['rst'])\n",
    "\n",
    "evaluation['filename'] = file\n",
    "cache.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from utils.train_test_split import split_train_dev_test\n",
    "from utils.file_reading import read_edus, read_gold, read_annotation\n",
    "from utils.evaluation import *\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "pipeline_evaluation = {}\n",
    "train, dev, test = split_train_dev_test('data/')\n",
    "\n",
    "cache = []\n",
    "thrown_error = []\n",
    "\n",
    "for file in tqdm(glob.glob('parsing_results_0707/*.pkl')):\n",
    "    \n",
    "    result = pickle.load(open(file, 'rb'))\n",
    "    file = file.replace('parsing_results_0707/', 'data/').replace('.pkl', '')\n",
    "\n",
    "    gold_edus = read_edus(file)\n",
    "    gold_pairs = prepare_gold_pairs(read_gold(file, features=True))\n",
    "\n",
    "    #evaluation = eval_pipeline(result['rst'], gold_edus, gold_pairs, result['text'])\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(result['rst'], result['text']), \n",
    "                                columns=['snippet_x', 'snippet_y', 'category_id', 'order'])\n",
    "    \n",
    "    evaluation = eval_pipeline(parsed_pairs=parsed_pairs,\n",
    "                           gold_edus=gold_edus,\n",
    "                           gold_pairs=gold_pairs,\n",
    "                           text=result['text'],\n",
    "                           trees=result['rst'])\n",
    "    \n",
    "    evaluation['filename'] = file\n",
    "    cache.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrown_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.train_test_split import split_train_dev_test\n",
    "from utils.file_reading import read_edus, read_gold\n",
    "from utils.evaluation import *\n",
    "import pandas as pd\n",
    "\n",
    "tmp = pd.DataFrame(cache)\n",
    "\n",
    "tmp['pr_seg'] = tmp.seg_true_pred / tmp.seg_all_pred\n",
    "tmp['re_seg'] = tmp.seg_true_pred / tmp.seg_all_true\n",
    "tmp['f1_seg'] = 2 * tmp.pr_seg * tmp.re_seg / (tmp.pr_seg + tmp.re_seg)\n",
    "tmp['pr_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_pred\n",
    "tmp['re_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_true\n",
    "tmp['f1_unlab'] = 2 * tmp.pr_unlab * tmp.re_unlab / (tmp.pr_unlab + tmp.re_unlab)\n",
    "tmp['pr_lab'] = tmp.lab_true_pred / tmp.lab_all_pred\n",
    "tmp['re_lab'] = tmp.lab_true_pred / tmp.lab_all_true\n",
    "tmp['f1_lab'] = 2 * tmp.pr_lab * tmp.re_lab / (tmp.pr_lab + tmp.re_lab)\n",
    "tmp['pr_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_pred\n",
    "tmp['re_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_true\n",
    "tmp['f1_nuc'] = 2 * tmp.pr_nuc * tmp.re_nuc / (tmp.pr_nuc + tmp.re_nuc)\n",
    "tmp['pr_full'] = tmp.full_true_pred / tmp.full_all_pred\n",
    "tmp['re_full'] = tmp.full_true_pred / tmp.full_all_true\n",
    "tmp['f1_full'] = 2 * tmp.pr_full * tmp.re_full / (tmp.pr_full + tmp.re_full)\n",
    "\n",
    "tmp.sort_values('f1_full', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.sort_values('f1_full', ascending=False)[[key for key in tmp.keys() if 'f1' in key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp.seg_all_true.sum(), tmp.seg_all_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp.lab_all_true.sum(), tmp.lab_all_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = tmp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp2[tmp2.filename.str.contains('blogs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_score = {\n",
    "    'pr_seg': tmp.seg_true_pred.sum() / tmp.seg_all_pred.sum(),\n",
    "    're_seg': tmp.seg_true_pred.sum() / tmp.seg_all_true.sum(),\n",
    "    'pr_unlab': tmp.unlab_true_pred.sum() / tmp.unlab_all_pred.sum(),\n",
    "    're_unlab': tmp.unlab_true_pred.sum() / tmp.unlab_all_true.sum(),\n",
    "    'pr_lab': tmp.lab_true_pred.sum() / tmp.lab_all_pred.sum(),\n",
    "    're_lab': tmp.lab_true_pred.sum() / tmp.lab_all_true.sum(),\n",
    "    'pr_nuc': tmp.nuc_true_pred.sum() / tmp.nuc_all_pred.sum(),\n",
    "    're_nuc': tmp.nuc_true_pred.sum() / tmp.nuc_all_true.sum(),\n",
    "    'pr_full': tmp.full_true_pred.sum() / tmp.full_all_pred.sum(),\n",
    "    're_full': tmp.full_true_pred.sum() / tmp.full_all_true.sum(),  \n",
    "}\n",
    "\n",
    "def get_overall_score(step: str):\n",
    "    return 2. * overall_score['pr_' + step] * overall_score['re_' + step] / (\n",
    "    overall_score['pr_' + step] + overall_score['re_' + step])\n",
    "\n",
    "for step in ('seg', 'unlab', 'nuc', 'lab', 'full'):\n",
    "    overall_score['f1_' + step] = get_overall_score(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_table = pd.DataFrame(columns=['component', 'P', 'R', 'F1'], data=[\n",
    "    ['segmentation', overall_score['pr_seg'], overall_score['re_seg'], overall_score['f1_seg']],\n",
    "    ['span', overall_score['pr_unlab'], overall_score['re_unlab'], overall_score['f1_unlab']],\n",
    "    ['nuclearity', overall_score['pr_nuc'], overall_score['re_nuc'], overall_score['f1_nuc']],\n",
    "    ['relation', overall_score['pr_lab'], overall_score['re_lab'], overall_score['f1_lab']],\n",
    "    ['full', overall_score['pr_full'], overall_score['re_full'], overall_score['f1_full']],\n",
    "])\n",
    "\n",
    "evaluation_table['P'] *= 100\n",
    "evaluation_table['R'] *= 100\n",
    "evaluation_table['F1'] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_table.to_latex(index=False, float_format='%.2f', column_format='|l|l|l|l|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(cache)\n",
    "tmp['pr_seg'] = tmp.seg_true_pred / tmp.seg_all_pred\n",
    "tmp['re_seg'] = tmp.seg_true_pred / tmp.seg_all_true\n",
    "tmp['f1_seg'] = 2 * tmp.pr_seg * tmp.re_seg / (tmp.pr_seg + tmp.re_seg)\n",
    "tmp['pr_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_pred\n",
    "tmp['re_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_true\n",
    "tmp['f1_unlab'] = 2 * tmp.pr_unlab * tmp.re_unlab / (tmp.pr_unlab + tmp.re_unlab)\n",
    "tmp['pr_lab'] = tmp.lab_true_pred / tmp.lab_all_pred\n",
    "tmp['re_lab'] = tmp.lab_true_pred / tmp.lab_all_true\n",
    "tmp['f1_lab'] = 2 * tmp.pr_lab * tmp.re_lab / (tmp.pr_lab + tmp.re_lab)\n",
    "tmp['pr_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_pred\n",
    "tmp['re_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_true\n",
    "tmp['f1_nuc'] = 2 * tmp.pr_nuc * tmp.re_nuc / (tmp.pr_nuc + tmp.re_nuc)\n",
    "tmp.sort_values('f1_unlab', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp.lab_all_true.sum(), tmp.lab_all_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_score = {\n",
    "    'pr_seg': tmp.seg_true_pred.sum() / tmp.seg_all_pred.sum(),\n",
    "    're_seg': tmp.seg_true_pred.sum() / tmp.seg_all_true.sum(),\n",
    "    'pr_unlab': tmp.unlab_true_pred.sum() / tmp.unlab_all_pred.sum(),\n",
    "    're_unlab': tmp.unlab_true_pred.sum() / tmp.unlab_all_true.sum(),\n",
    "    'pr_lab': tmp.lab_true_pred.sum() / tmp.lab_all_pred.sum(),\n",
    "    're_lab': tmp.lab_true_pred.sum() / tmp.lab_all_true.sum(),\n",
    "    'pr_nuc': tmp.nuc_true_pred.sum() / tmp.nuc_all_pred.sum(),\n",
    "    're_nuc': tmp.nuc_true_pred.sum() / tmp.nuc_all_true.sum(),\n",
    "}\n",
    "\n",
    "overall_score['f1_seg'] = 2. * overall_score['pr_seg'] * overall_score['re_seg'] / (\n",
    "    overall_score['pr_seg'] + overall_score['re_seg'])\n",
    "overall_score['f1_unlab'] = 2. * overall_score['pr_unlab'] * overall_score['re_unlab'] / (\n",
    "    overall_score['pr_unlab'] + overall_score['re_unlab'])\n",
    "overall_score['f1_lab'] = 2. * overall_score['pr_lab'] * overall_score['re_lab'] / (\n",
    "    overall_score['pr_lab'] + overall_score['re_lab'])\n",
    "overall_score['f1_nuc'] = 2. * overall_score['pr_nuc'] * overall_score['re_nuc'] / (\n",
    "    overall_score['pr_nuc'] + overall_score['re_nuc'])\n",
    "\n",
    "overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_table = pd.DataFrame(columns=['component', 'P', 'R', 'F1'], data=[\n",
    "    ['segmentation', overall_score['pr_seg'], overall_score['re_seg'], overall_score['f1_seg']],\n",
    "    ['span', overall_score['pr_unlab'], overall_score['re_unlab'], overall_score['f1_unlab']],\n",
    "    ['nuclearity', overall_score['pr_nuc'], overall_score['re_nuc'], overall_score['f1_nuc']],\n",
    "    ['relation', overall_score['pr_lab'], overall_score['re_lab'], overall_score['f1_lab']],\n",
    "])\n",
    "\n",
    "evaluation_table['P'] *= 100\n",
    "evaluation_table['R'] *= 100\n",
    "evaluation_table['F1'] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_table.to_latex(index=False, float_format='%.2f', column_format='|l|l|l|l|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from utils.train_test_split import split_train_dev_test\n",
    "from utils.file_reading import read_edus, read_gold\n",
    "from utils.evaluation import *\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "for i in range(len(test[:1])):\n",
    "    file = glob.glob('parsing_results_crf/*.pkl')[i]\n",
    "    result = pickle.load(open(file, 'rb'))\n",
    "    file = file.replace('parsing_results_crf/', 'data/')\n",
    "    file = file.replace('.pkl', '')\n",
    "\n",
    "    gold_edus = read_edus(file)\n",
    "    gold_pairs = prepare_gold_pairs(read_gold(file, features=True))\n",
    "    pred_edus = [] \n",
    "    for tree in result['rst']:\n",
    "        pred_edus += extr_edus(tree)\n",
    "\n",
    "    evaluation = eval_pipeline(result['rst'], gold_edus, gold_pairs, result['text'])\n",
    "    evaluation['filename'] = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gold_edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame([evaluation])\n",
    "tmp['pr_seg'] = tmp.seg_true_pred / tmp.seg_all_pred\n",
    "tmp['re_seg'] = tmp.seg_true_pred / tmp.seg_all_true\n",
    "tmp['f1_seg'] = 2 * tmp.pr_seg * tmp.re_seg / (tmp.pr_seg + tmp.re_seg)\n",
    "tmp['pr_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_pred\n",
    "tmp['re_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_true\n",
    "tmp['f1_unlab'] = 2 * tmp.pr_unlab * tmp.re_unlab / (tmp.pr_unlab + tmp.re_unlab)\n",
    "tmp['pr_lab'] = tmp.lab_true_pred / tmp.lab_all_pred\n",
    "tmp['re_lab'] = tmp.lab_true_pred / tmp.lab_all_true\n",
    "tmp['f1_lab'] = 2 * tmp.pr_lab * tmp.re_lab / (tmp.pr_lab + tmp.re_lab)\n",
    "tmp['pr_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_pred\n",
    "tmp['re_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_true\n",
    "tmp['f1_nuc'] = 2 * tmp.pr_nuc * tmp.re_nuc / (tmp.pr_nuc + tmp.re_nuc)\n",
    "tmp.sort_values('f1_seg', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
