{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhetorical relations classification used in tree building: ESIM\n",
    "\n",
    "Prepare data and model-related scripts.\n",
    "\n",
    "Evaluate models.\n",
    "\n",
    "Make and evaluate ansembles for ESIM and BiMPM model / ESIM and feature-based model.\n",
    "\n",
    "Output:\n",
    " - ``models/relation_predictor_esim/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.file_reading import read_edus, read_gold, read_negative, read_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/label_predictor_esim'\n",
    "! mkdir $MODEL_PATH\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(MODEL_PATH, 'nlabel_cf_train.tsv')\n",
    "DEV_FILE_PATH = os.path.join(MODEL_PATH, 'nlabel_cf_dev.tsv')\n",
    "TEST_FILE_PATH = os.path.join(MODEL_PATH, 'nlabel_cf_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_labeling'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train_samples['relation'].value_counts(normalize=False).values\n",
    "NUMBER_CLASSES = len(counts)\n",
    "print(\"number of classes:\", NUMBER_CLASSES)\n",
    "print(\"class weights:\")\n",
    "np.round(counts.min() / counts, decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train_samples['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_samples.reset_index()\n",
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'index']].to_csv(TRAIN_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "dev_samples = dev_samples.reset_index()\n",
    "dev_samples[['relation', 'snippet_x', 'snippet_y', 'index']].to_csv(DEV_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "test_samples = test_samples.reset_index()\n",
    "test_samples[['relation', 'snippet_x', 'snippet_y', 'index']].to_csv(TEST_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Add F1, concatenated encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/bimpm_custom_package/model/esim.py\n",
    "\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.models.esim import ESIM\n",
    "from allennlp.modules import FeedForward, InputVariationalDropout\n",
    "from allennlp.modules.matrix_attention.legacy_matrix_attention import LegacyMatrixAttention\n",
    "from allennlp.modules.similarity_functions.similarity_function import SimilarityFunction\n",
    "from allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\n",
    "from allennlp.nn.initializers import InitializerApplicator\n",
    "from allennlp.nn.regularizers import RegularizerApplicator\n",
    "from allennlp.nn.util import (\n",
    "    get_text_field_mask,\n",
    "    masked_softmax,\n",
    "    weighted_sum,\n",
    "    masked_max,\n",
    "    replace_masked_values,\n",
    ")\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "\n",
    "\n",
    "@Model.register(\"custom_esim\")\n",
    "class CustomESIM(Model):\n",
    "    \"\"\"\n",
    "    This `Model` implements the ESIM sequence model described in [Enhanced LSTM for Natural Language Inference]\n",
    "    (https://api.semanticscholar.org/CorpusID:34032948) by Chen et al., 2017.\n",
    "    Registered as a `Model` with name \"esim\".\n",
    "    # Parameters\n",
    "    vocab : `Vocabulary`\n",
    "    text_field_embedder : `TextFieldEmbedder`\n",
    "        Used to embed the `premise` and `hypothesis` `TextFields` we get as input to the\n",
    "        model.\n",
    "    encoder : `Seq2SeqEncoder`\n",
    "        Used to encode the premise and hypothesis.\n",
    "    matrix_attention : `MatrixAttention`\n",
    "        This is the attention function used when computing the similarity matrix between encoded\n",
    "        words in the premise and words in the hypothesis.\n",
    "    projection_feedforward : `FeedForward`\n",
    "        The feedforward network used to project down the encoded and enhanced premise and hypothesis.\n",
    "    inference_encoder : `Seq2SeqEncoder`\n",
    "        Used to encode the projected premise and hypothesis for prediction.\n",
    "    output_feedforward : `FeedForward`\n",
    "        Used to prepare the concatenated premise and hypothesis for prediction.\n",
    "    output_logit : `FeedForward`\n",
    "        This feedforward network computes the output logits.\n",
    "    dropout : `float`, optional (default=`0.5`)\n",
    "        Dropout percentage to use.\n",
    "    initializer : `InitializerApplicator`, optional (default=`InitializerApplicator()`)\n",
    "        Used to initialize the model parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Vocabulary,\n",
    "        text_field_embedder: TextFieldEmbedder,\n",
    "        encoder: Seq2SeqEncoder,\n",
    "        similarity_function: SimilarityFunction,\n",
    "        projection_feedforward: FeedForward,\n",
    "        inference_encoder: Seq2SeqEncoder,\n",
    "        output_feedforward: FeedForward,\n",
    "        output_logit: FeedForward,\n",
    "        dropout: float = 0.5,\n",
    "        class_weights: list = [],\n",
    "        initializer: InitializerApplicator = InitializerApplicator(),\n",
    "        regularizer: Optional[RegularizerApplicator] = None,\n",
    "        encode_together: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(vocab, regularizer)\n",
    "\n",
    "        self._text_field_embedder = text_field_embedder\n",
    "        self._encoder = encoder\n",
    "        self.encode_together = encode_together\n",
    "\n",
    "        self._matrix_attention = LegacyMatrixAttention(similarity_function)\n",
    "        self._projection_feedforward = projection_feedforward\n",
    "\n",
    "        self._inference_encoder = inference_encoder\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "            self.rnn_input_dropout = InputVariationalDropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "            self.rnn_input_dropout = None\n",
    "            \n",
    "        if class_weights:\n",
    "            self.class_weights = class_weights\n",
    "        else:\n",
    "            self.class_weights = [1.] * self.output_feedforward.get_output_dim()\n",
    "\n",
    "        self._output_feedforward = output_feedforward\n",
    "        self._output_logit = output_logit\n",
    "\n",
    "        self._num_labels = vocab.get_vocab_size(namespace=\"labels\")\n",
    "\n",
    "        check_dimensions_match(\n",
    "            text_field_embedder.get_output_dim(),\n",
    "            encoder.get_input_dim(),\n",
    "            \"text field embedding dim\",\n",
    "            \"encoder input dim\",\n",
    "        )\n",
    "        check_dimensions_match(\n",
    "            encoder.get_output_dim() * 4,\n",
    "            projection_feedforward.get_input_dim(),\n",
    "            \"encoder output dim\",\n",
    "            \"projection feedforward input\",\n",
    "        )\n",
    "        check_dimensions_match(\n",
    "            projection_feedforward.get_output_dim(),\n",
    "            inference_encoder.get_input_dim(),\n",
    "            \"proj feedforward output dim\",\n",
    "            \"inference lstm input dim\",\n",
    "        )\n",
    "\n",
    "        self.metrics = {\"accuracy\": CategoricalAccuracy()}\n",
    "        \n",
    "        for _class in range(len(self.class_weights)):\n",
    "            self.metrics.update({\n",
    "                f\"f1_rel{_class}\": F1Measure(_class),\n",
    "            })\n",
    "        \n",
    "        self._loss = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(self.class_weights))\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    def forward(  # type: ignore\n",
    "        self,\n",
    "        premise: Dict[str, torch.LongTensor],\n",
    "        hypothesis: Dict[str, torch.LongTensor],\n",
    "        label: torch.IntTensor = None,\n",
    "        metadata: List[Dict[str, Any]] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        # Parameters\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            From a `TextField`\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            From a `TextField`\n",
    "        label : `torch.IntTensor`, optional (default = `None`)\n",
    "            From a `LabelField`\n",
    "        metadata : `List[Dict[str, Any]]`, optional (default = `None`)\n",
    "            Metadata containing the original tokenization of the premise and\n",
    "            hypothesis with 'premise_tokens' and 'hypothesis_tokens' keys respectively.\n",
    "        # Returns\n",
    "        An output dictionary consisting of:\n",
    "        label_logits : `torch.FloatTensor`\n",
    "            A tensor of shape `(batch_size, num_labels)` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        label_probs : `torch.FloatTensor`\n",
    "            A tensor of shape `(batch_size, num_labels)` representing probabilities of the\n",
    "            entailment label.\n",
    "        loss : `torch.FloatTensor`, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "        \n",
    "        def encode_pair(x1, x2, mask1=None, mask2=None):\n",
    "            _joined_pair: Dict[str, torch.LongTensor] = {}\n",
    "            \n",
    "            for key in premise.keys():\n",
    "                bsz = premise[key].size(0)\n",
    "                x1_len, x2_len = premise[key].size(1), hypothesis[key].size(1)\n",
    "                sep = torch.empty([bsz, 1], dtype=torch.long, device=premise[key].device)\n",
    "                sep.data.fill_(0) # 2 is the id for </s>\n",
    "                \n",
    "                x = torch.cat([premise[key], hypothesis[key]], dim=1)\n",
    "                _joined_pair[key] = x\n",
    "                \n",
    "            x_output = self.dropout(self._text_field_embedder(_joined_pair))\n",
    "            return x_output[:, :x1_len], x_output[:, -x2_len:], mask1, mask2\n",
    "        \n",
    "        premise_mask = get_text_field_mask(premise)\n",
    "        hypothesis_mask = get_text_field_mask(hypothesis)\n",
    "        \n",
    "        if self.encode_together:\n",
    "            embedded_premise, embedded_hypothesis, _, _ = encode_pair(premise, hypothesis)\n",
    "        else:\n",
    "            embedded_premise = self.dropout(self._text_field_embedder(premise))\n",
    "            embedded_hypothesis = self.dropout(self._text_field_embedder(hypothesis))\n",
    "\n",
    "        # apply dropout for LSTM\n",
    "        if self.rnn_input_dropout:\n",
    "            embedded_premise = self.rnn_input_dropout(embedded_premise)\n",
    "            embedded_hypothesis = self.rnn_input_dropout(embedded_hypothesis)\n",
    "\n",
    "        # encode premise and hypothesis\n",
    "        encoded_premise = self._encoder(embedded_premise, premise_mask)\n",
    "        encoded_hypothesis = self._encoder(embedded_hypothesis, hypothesis_mask)\n",
    "\n",
    "        # Shape: (batch_size, premise_length, hypothesis_length)\n",
    "        similarity_matrix = self._matrix_attention(encoded_premise, encoded_hypothesis)\n",
    "\n",
    "        # Shape: (batch_size, premise_length, hypothesis_length)\n",
    "        p2h_attention = masked_softmax(similarity_matrix, hypothesis_mask)\n",
    "        # Shape: (batch_size, premise_length, embedding_dim)\n",
    "        attended_hypothesis = weighted_sum(encoded_hypothesis, p2h_attention)\n",
    "\n",
    "        # Shape: (batch_size, hypothesis_length, premise_length)\n",
    "        h2p_attention = masked_softmax(similarity_matrix.transpose(1, 2).contiguous(), premise_mask)\n",
    "        # Shape: (batch_size, hypothesis_length, embedding_dim)\n",
    "        attended_premise = weighted_sum(encoded_premise, h2p_attention)\n",
    "\n",
    "        # the \"enhancement\" layer\n",
    "        premise_enhanced = torch.cat(\n",
    "            [encoded_premise, attended_hypothesis,\n",
    "             encoded_premise - attended_hypothesis,\n",
    "             encoded_premise * attended_hypothesis,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        hypothesis_enhanced = torch.cat(\n",
    "            [encoded_hypothesis, attended_premise,\n",
    "             encoded_hypothesis - attended_premise,\n",
    "             encoded_hypothesis * attended_premise,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        # The projection layer down to the model dimension.  Dropout is not applied before\n",
    "        # projection.\n",
    "        projected_enhanced_premise = self._projection_feedforward(premise_enhanced)\n",
    "        projected_enhanced_hypothesis = self._projection_feedforward(hypothesis_enhanced)\n",
    "\n",
    "        # Run the inference layer\n",
    "        if self.rnn_input_dropout:\n",
    "            projected_enhanced_premise = self.rnn_input_dropout(projected_enhanced_premise)\n",
    "            projected_enhanced_hypothesis = self.rnn_input_dropout(projected_enhanced_hypothesis)\n",
    "        v_ai = self._inference_encoder(projected_enhanced_premise, premise_mask)\n",
    "        v_bi = self._inference_encoder(projected_enhanced_hypothesis, hypothesis_mask)\n",
    "\n",
    "        # The pooling layer -- max and avg pooling.\n",
    "        # (batch_size, model_dim)\n",
    "        v_a_max, _ = replace_masked_values(v_ai, premise_mask.unsqueeze(-1), -1e7).max(dim=1)\n",
    "        v_b_max, _ = replace_masked_values(v_bi, hypothesis_mask.unsqueeze(-1), -1e7).max(dim=1)\n",
    "\n",
    "        v_a_avg = torch.sum(v_ai * premise_mask.unsqueeze(-1), dim=1) / torch.sum(\n",
    "            premise_mask, 1, keepdim=True\n",
    "        )\n",
    "        v_b_avg = torch.sum(v_bi * hypothesis_mask.unsqueeze(-1), dim=1) / torch.sum(\n",
    "            hypothesis_mask, 1, keepdim=True\n",
    "        )\n",
    "\n",
    "        # Now concat\n",
    "        # (batch_size, model_dim * 2 * 4)\n",
    "        v_all = torch.cat([v_a_avg, v_a_max, v_b_avg, v_b_max], dim=1)\n",
    "\n",
    "        # the final MLP -- apply dropout to input, and MLP applies to output & hidden\n",
    "        if self.dropout:\n",
    "            v_all = self.dropout(v_all)\n",
    "\n",
    "        output_hidden = self._output_feedforward(v_all)\n",
    "        label_logits = self._output_logit(output_hidden)\n",
    "        label_probs = torch.nn.functional.softmax(label_logits, dim=-1)\n",
    "\n",
    "        output_dict = {\"label_logits\": label_logits, \"label_probs\": label_probs}\n",
    "\n",
    "        if label is not None:\n",
    "            loss = self._loss(label_logits, label.long().view(-1))\n",
    "            output_dict[\"loss\"] = loss\n",
    "            \n",
    "            for metric in self.metrics.values():\n",
    "                metric(label_logits, label.long().view(-1))\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metrics = {\"accuracy\": self.metrics[\"accuracy\"].get_metric(reset=reset)}\n",
    "        \n",
    "        for _class in range(len(self.class_weights)):\n",
    "            metrics.update({\n",
    "                f\"f1_rel{_class}\": self.metrics[f\"f1_rel{_class}\"].get_metric(reset=reset)[2],\n",
    "            })\n",
    "        \n",
    "        metrics[\"f1_macro\"] = numpy.mean([metrics[f\"f1_rel{_class}\"] for _class in range(len(self.class_weights))])\n",
    "        return metrics\n",
    "\n",
    "    default_predictor = \"textual_entailment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp models/bimpm_custom_package/model/esim.py ../../../maintenance_rst/models/customization_package/model/esim.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate config files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELMo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $MODEL_PATH/config_elmo.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"quora_paraphrase\",\n",
    "    \"lazy\": false,\n",
    "    \"tokenizer\": {\n",
    "      \"type\": \"word\",\n",
    "      \"word_splitter\": {\n",
    "        \"type\": \"just_spaces\"\n",
    "      }\n",
    "    },\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 30,\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"label_predictor_esim/nlabel_cf_train.tsv\",\n",
    "  \"validation_data_path\": \"label_predictor_esim/nlabel_cf_dev.tsv\",\n",
    "  \"test_data_path\": \"label_predictor_esim/nlabel_cf_test.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"bidaf\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"class_weights\": [\n",
    "        0.022915, 0.027624, 0.069509, 0.104457, 0.109012, 0.111773,\n",
    "        0.123355, 0.139147, 0.153374, 0.157233, 0.159915, 0.16129 ,\n",
    "        0.191327, 0.202703, 0.288462, 0.337838, 0.347222, 0.535714,\n",
    "        0.630252, 0.757576, 0.806452, 1.0      ],\n",
    "    \"encode_together\": true,\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"elmo\": {\n",
    "                    \"type\": \"elmo_token_embedder\",\n",
    "                    \"options_file\": \"rsv_elmo/options.json\",\n",
    "                    \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "                    \"do_layer_norm\": false,\n",
    "                    \"dropout\": 0.1\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"dropout\": 0.1,\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"padding_index\": 0,\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"gru\",\n",
    "                    \"input_size\": 20,\n",
    "                    \"hidden_size\": 50,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true,\n",
    "                },\n",
    "            },\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"input_size\": 1024+100,\n",
    "      \"hidden_size\": 300,\n",
    "      \"num_layers\": 1,\n",
    "      \"bidirectional\": true\n",
    "    },\n",
    "    \"similarity_function\": {\"type\": \"dot_product\"},\n",
    "    \"projection_feedforward\": {\n",
    "      \"input_dim\": 2400,\n",
    "      \"hidden_dims\": 300,\n",
    "      \"num_layers\": 1,\n",
    "      \"activations\": \"relu\"\n",
    "    },\n",
    "    \"inference_encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"input_size\": 300,\n",
    "      \"hidden_size\": 300,\n",
    "      \"num_layers\": 1,\n",
    "      \"bidirectional\": true\n",
    "    },\n",
    "    \"output_feedforward\": {\n",
    "      \"input_dim\": 2400,\n",
    "      \"num_layers\": 1,\n",
    "      \"hidden_dims\": 300,\n",
    "      \"activations\": \"relu\",\n",
    "      \"dropout\": 0.5\n",
    "    },\n",
    "    \"output_logit\": {\n",
    "      \"input_dim\": 300,\n",
    "      \"num_layers\": 1,\n",
    "      \"hidden_dims\": 22,\n",
    "      \"activations\": \"linear\"\n",
    "    },\n",
    "     \"initializer\": [\n",
    "      [\".*linear_layers.*weight\", {\"type\": \"xavier_uniform\"}],\n",
    "      [\".*linear_layers.*bias\", {\"type\": \"zero\"}],\n",
    "      [\".*weight_ih.*\", {\"type\": \"xavier_uniform\"}],\n",
    "      [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "      [\".*bias_ih.*\", {\"type\": \"zero\"}],\n",
    "      [\".*bias_hh.*\", {\"type\": \"lstm_hidden_bias\"}]\n",
    "     ]\n",
    "   },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"bucket\",\n",
    "    \"padding_noise\": 0,\n",
    "    \"sorting_keys\": [[\"premise\", \"num_tokens\"], [\"hypothesis\", \"num_tokens\"]],\n",
    "    \"batch_size\": 4\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 200,\n",
    "    \"cuda_device\": 0\n",
    "    \"shuffle\": true,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"lr\": 0.001\n",
    "    },\n",
    "    \"type\":\"callback\",\n",
    "    \"callbacks\":[\n",
    "        {\n",
    "            \"type\": \"validate\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"checkpoint\",\n",
    "            \"checkpointer\":{\n",
    "                \"num_serialized_models_to_keep\":1\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gradient_norm_and_clip\", \n",
    "            \"grad_norm\": 5.0\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"track_metrics\",\n",
    "            \"patience\": 20,\n",
    "            \"validation_metric\": \"+f1_macro\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"log_metrics_to_wandb\"\n",
    "        }\n",
    "    ],\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r $MODEL_PATH ../../../maintenance_rst/models/label_predictor_esim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r $MODEL_PATH/config_elmo.json ../../../maintenance_rst/models/label_predictor_esim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scripts for training/prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1. Directly from the config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/train_label_predictor_esim.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_label_predictor.sh {bert|elmo} result_30\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"nlabel_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"nlabel_cf_test.tsv\"\n",
    "\n",
    "rm -r label_predictor_bimpm/${RESULT_DIR}/\n",
    "allennlp train -s label_predictor_esim/${RESULT_DIR}/ label_predictor_esim/config_${METHOD}.json \\\n",
    "    --include-package customization_package\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_esim/${RESULT_DIR}/predictions_dev.json label_predictor_esim/${RESULT_DIR}/model.tar.gz label_predictor_esim/${DEV_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_esim/${RESULT_DIR}/predictions_test.json label_predictor_esim/${RESULT_DIR}/model.tar.gz label_predictor_esim/${TEST_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp models/train_label_predictor_esim.sh ../../../maintenance_rst/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on dev&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/eval_label_predictor_esim.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_label_predictor.sh {bert|elmo} result_30\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"nlabel_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"nlabel_cf_test.tsv\"\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_esim/${RESULT_DIR}/predictions_dev.json label_predictor_esim/${RESULT_DIR}/model.tar.gz label_predictor_esim/${DEV_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_esim/${RESULT_DIR}/predictions_test.json label_predictor_esim/${RESULT_DIR}/model.tar.gz label_predictor_esim/${TEST_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp models/eval_label_predictor_esim.sh ../../../maintenance_rst/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) predict on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/eval_label_predictor_train.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh eval_label_predictor_train.sh {bert|elmo} result_30\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export TEST_FILE_PATH=\"nlabel_cf_train.tsv\"\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file label_predictor_bimpm/${RESULT_DIR}/predictions_train.json label_predictor_bimpm/${RESULT_DIR}/model.tar.gz label_predictor_bimpm/${TEST_FILE_PATH} \\\n",
    "    --include-package customization_package \\\n",
    "    --predictor textual-entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Using wandb for parameters adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile ../../../maintenance_rst/models/wandb_label_predictor_esim.yaml\n",
    "\n",
    "name: label_predictor_esim\n",
    "program: wandb_allennlp # this is a wrapper console script around allennlp commands. It is part of wandb-allennlp\n",
    "method: bayes\n",
    "## Do not for get to use the command keyword to specify the following command structure\n",
    "command:\n",
    "  - ${program} #omit the interpreter as we use allennlp train command directly\n",
    "  - \"--subcommand=train\"\n",
    "  - \"--include-package=customization_package\" # add all packages containing your registered classes here\n",
    "  - \"--config_file=label_predictor_esim/config_elmo.json\"\n",
    "  - ${args}\n",
    "metric:\n",
    "    name: best_f1_macro\n",
    "    goal: maximize\n",
    "parameters:\n",
    "    model.encode_together:\n",
    "        values: [\"true\", ]    \n",
    "    iterator.batch_size:\n",
    "        values: [8,]\n",
    "    trainer.optimizer.lr:\n",
    "        values: [0.001,]\n",
    "    model.dropout:\n",
    "        values: [0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``wandb sweep wandb_label_predictor_esim.yaml``\n",
    "\n",
    "(returns %sweepname1)\n",
    "\n",
    "``wandb sweep wandb_label_predictor2.yaml``\n",
    "\n",
    "(returns %sweepname2)\n",
    "\n",
    "``wandb agent --count 1 %sweepname1 && wandb agent --count 1 %sweepname2``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the best model in label_predictor_bimpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls -laht models/wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/wandb/run-20201218_123424-kcphaqhi/training_dumps models/label_predictor_esim/esim_elmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or** load from wandb by %sweepname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "run = api.run(\"tchewik/tmp/7hum4oom\")\n",
    "for file in run.files():\n",
    "    file.download(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r training_dumps models/label_predictor_bimpm/toasty-sweep-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run evaluation from shell\n",
    "\n",
    "``sh eval_label_predictor_esim.sh {elmo|elmo_fasttext} toasty-sweep-1``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(path):\n",
    "    result = []\n",
    "    vocab = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            line = json.loads(line)\n",
    "            if line.get(\"label\"):\n",
    "                result.append(line.get(\"label\"))\n",
    "            elif line.get(\"label_probs\"):\n",
    "                if not vocab:\n",
    "                    vocab = open(path[:path.rfind('/')] + '/vocabulary/labels.txt', 'r').readlines()\n",
    "                    vocab = [label.strip() for label in vocab]\n",
    "                \n",
    "                result.append(vocab[np.argmax(line.get(\"label_probs\"))])\n",
    "            \n",
    "    print('length of result:', len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = 'esim_elmo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/label_predictor_esim/$RESULT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r ../../../maintenance_rst/models/label_predictor_esim/$RESULT_DIR/*.json models/label_predictor_esim/$RESULT_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(DEV_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = classification_report(true[:len(pred)], pred, digits=4, output_dict=True)\n",
    "test_f1 = np.array(\n",
    "    [test_metrics[label].get('f1-score') for label in test_metrics if type(test_metrics[label]) == dict]) * 100\n",
    "\n",
    "test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred, average='macro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_confusion_matrix import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = list(set(true))\n",
    "labels.sort()\n",
    "plot_confusion_matrix(confusion_matrix(true[:len(pred)], pred, labels), target_names=labels, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_classes = [\n",
    "    'attribution_NS',\n",
    "    'attribution_SN',\n",
    "    'purpose_NS',\n",
    "    'purpose_SN',\n",
    "    'condition_SN',\n",
    "    'contrast_NN',\n",
    "    'condition_NS',\n",
    "    'joint_NN',\n",
    "    'concession_NS',\n",
    "    'same-unit_NN',\n",
    "    'elaboration_NS',\n",
    "    'cause-effect_NS',\n",
    "]\n",
    "\n",
    "class_mapper = {weird_class: 'other' + weird_class[-3:] for weird_class in labels if not weird_class in top_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "true = [class_mapper.get(value) if class_mapper.get(value) else value for value in true]\n",
    "pred = [class_mapper.get(value) if class_mapper.get(value) else value for value in pred]\n",
    "\n",
    "pred_mapper = {\n",
    "    'other_NN': 'joint_NN',\n",
    "    'other_NS': 'joint_NN',\n",
    "    'other_SN': 'joint_NN'\n",
    "}\n",
    "pred = [pred_mapper.get(value) if pred_mapper.get(value) else value for value in pred]\n",
    "\n",
    "_to_stay = (np.array(true) != 'other_NN') & (np.array(true) != 'other_SN') & (np.array(true) != 'other_NS')\n",
    "\n",
    "_true = np.array(true)[_to_stay]\n",
    "_pred = np.array(pred)[_to_stay[:len(pred)]]\n",
    "labels = list(set(_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred, average='macro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(_true[:len(_pred)], _pred), target_names=labels, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for rel in np.unique(_true):\n",
    "    print(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On train set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv('models/label_predictor_bimpm/nlabel_cf_train.tsv', sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_train.json')\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = 'models/label_predictor_lstm/nlabel_cf_train.tsv'\n",
    "true_train = pd.read_csv(file, sep='\\t', header=None)\n",
    "true_train['predicted_relation'] = pred\n",
    "\n",
    "print(true_train[true_train.relation != true_train.predicted_relation].shape)\n",
    "\n",
    "true_train[true_train.relation != true_train.predicted_relation].to_csv('mispredicted_relations.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(TEST_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = classification_report(true[:len(pred)], pred, digits=4, output_dict=True)\n",
    "test_f1 = np.array(\n",
    "    [test_metrics[label].get('f1-score') for label in test_metrics if type(test_metrics[label]) == dict]) * 100\n",
    "\n",
    "test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred, average='macro')*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred, average='macro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = [class_mapper.get(value) if class_mapper.get(value) else value for value in true]\n",
    "pred = [class_mapper.get(value) if class_mapper.get(value) else value for value in pred]\n",
    "pred = [pred_mapper.get(value) if pred_mapper.get(value) else value for value in pred]\n",
    "\n",
    "_to_stay = (np.array(true) != 'other_NN') & (np.array(true) != 'other_SN') & (np.array(true) != 'other_NS')\n",
    "\n",
    "_true = np.array(true)[_to_stay]\n",
    "_pred = np.array(pred)[_to_stay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(_true[:len(_pred)], _pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(_true[:len(_pred)], _pred, average='macro')*100))\n",
    "print('pr: %.2f'%(precision_score(_true[:len(_pred)], _pred, average='macro')*100))\n",
    "print('re: %.2f'%(recall_score(_true[:len(_pred)], _pred, average='macro')*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: (Logreg+Catboost) + ESIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls models/label_predictor_esim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_vocab = open(MODEL_PATH + '/' + RESULT_DIR + '/vocabulary/labels.txt', 'r').readlines()\n",
    "model_vocab = [label.strip() for label in model_vocab]\n",
    "\n",
    "catboost_vocab = [\n",
    "   'attribution_NS', 'attribution_SN', 'background_NS',\n",
    "   'cause-effect_NS', 'cause-effect_SN', 'comparison_NN',\n",
    "   'concession_NS', 'condition_NS', 'condition_SN', 'contrast_NN',\n",
    "   'elaboration_NS', 'evidence_NS', 'interpretation-evaluation_NS',\n",
    "   'interpretation-evaluation_SN', 'joint_NN', 'preparation_SN',\n",
    "   'purpose_NS', 'purpose_SN', 'restatement_NN', 'same-unit_NN',\n",
    "   'sequence_NN', 'solutionhood_SN']\n",
    "\n",
    "def load_neural_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            line = json.loads(line)\n",
    "            if line.get('probs'):\n",
    "                probs = line.get('probs')\n",
    "            elif line.get('label_probs'):\n",
    "                probs = line.get('label_probs')\n",
    "            probs = {model_vocab[i]: probs[i] for i in range(len(model_vocab))}\n",
    "            result.append(probs)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def load_scikit_predictions(model, X):\n",
    "    result = []\n",
    "    predictions = model.predict_proba(X)\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        probs = {catboost_vocab[j]: prediction[j] for j in range(len(catboost_vocab))}\n",
    "        result.append(probs)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vote_predictions(predictions, soft=True, weights=[1., 1.]):\n",
    "    for i in range(1, len(predictions)):\n",
    "        assert len(predictions[i-1]) == len(predictions[i])\n",
    "        \n",
    "    if weights == [1., 1.]:\n",
    "        weights = [1.,] * len(predictions)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(predictions[0])):\n",
    "        sample_result = {}\n",
    "        for key in predictions[0][i].keys():\n",
    "            if soft:\n",
    "                sample_result[key] = 0\n",
    "                for j, prediction in enumerate(predictions):\n",
    "                    sample_result[key] += prediction[i][key] * weights[j]\n",
    "            else:\n",
    "                sample_result[key] = max([pred[i][key] * weights[j] for j, pred in enumerate(predictions)])\n",
    "\n",
    "        \n",
    "        result.append(sample_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def probs_to_classes(pred):\n",
    "    result = []\n",
    "    \n",
    "    for sample in pred:\n",
    "        best_class = ''\n",
    "        best_prob = 0.\n",
    "        for key in sample.keys():\n",
    "            if sample[key] > best_prob:\n",
    "                best_prob = sample[key]\n",
    "                best_class = key\n",
    "        \n",
    "        result.append(best_class)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fs_catboost_plus_logreg = pickle.load(open('models/relation_predictor_baseline/model.pkl', 'rb'))\n",
    "lab_encoder = pickle.load(open('models/relation_predictor_baseline/label_encoder.pkl', 'rb'))\n",
    "scaler = pickle.load(open('models/relation_predictor_baseline/scaler.pkl', 'rb'))\n",
    "drop_columns = pickle.load(open('models/relation_predictor_baseline/drop_columns.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "TARGET = 'relation'\n",
    "\n",
    "y_dev, X_dev = dev_samples['relation'].to_frame(), dev_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id', 'index'])\n",
    "\n",
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)\n",
    "\n",
    "catboost_predictions = load_scikit_predictions(fs_catboost_plus_logreg, X_dev)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, catboost_predictions, soft=True, weights=[1., 1.])\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_dev.values, ensemble_pred, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_dev.values, ensemble_pred, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_dev.values, ensemble_pred))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'relation'\n",
    "\n",
    "y_test, X_test = test_samples[TARGET].to_frame(), test_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id', 'index'])\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)\n",
    "\n",
    "catboost_predictions = load_scikit_predictions(fs_catboost_plus_logreg, X_test)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, catboost_predictions, soft=True, weights=[1., 2.])\n",
    "\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_test.values, ensemble_pred, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_test.values, ensemble_pred, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_test.values, ensemble_pred))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = metrics.classification_report(y_test, ensemble_pred, digits=4, output_dict=True)\n",
    "test_f1 = np.array(\n",
    "    [test_metrics[label].get('f1-score') for label in test_metrics if type(test_metrics[label]) == dict]) * 100\n",
    "\n",
    "test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: BiMPM + ESIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls models/label_predictor_bimpm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "TARGET = 'relation'\n",
    "\n",
    "y_dev, X_dev = dev_samples['relation'].to_frame(), dev_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id', 'index'])\n",
    "\n",
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)\n",
    "\n",
    "bimpm = load_neural_predictions(f'models/label_predictor_bimpm/winter-sweep-1/predictions_dev.json')\n",
    "esim = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "catboost_predictions = load_scikit_predictions(fs_catboost_plus_logreg, X_dev)\n",
    "\n",
    "tmp = vote_predictions(bimpm, esim, soft=False, weights=[1., 1.])\n",
    "tmp = vote_predictions(tmp, catboost_predictions, soft=True, weights=[1., 1.])\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_dev.values, ensemble_pred, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_dev.values, ensemble_pred, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_dev.values, ensemble_pred))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'relation'\n",
    "\n",
    "y_test, X_test = test_samples[TARGET].to_frame(), test_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id', 'index'])\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)\n",
    "\n",
    "bimpm = load_neural_predictions(f'models/label_predictor_bimpm/winter-sweep-1/predictions_test.json')\n",
    "esim = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "catboost_predictions = load_scikit_predictions(fs_catboost_plus_logreg, X_test)\n",
    "\n",
    "tmp = vote_predictions([bimpm, catboost_predictions, esim], soft=True, weights=[2., 1, 15.])\n",
    "\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_test.values, ensemble_pred, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_test.values, ensemble_pred, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_test.values, ensemble_pred))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, ensemble_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
